<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | HumblePoster</title>
    <link>https://p0st3r.github.io/ml/</link>
      <atom:link href="https://p0st3r.github.io/ml/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>LithiumÂ©2016</copyright><lastBuildDate>Sun, 09 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://p0st3r.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Machine Learning</title>
      <link>https://p0st3r.github.io/ml/</link>
    </image>
    
    <item>
      <title>Definition</title>
      <link>https://p0st3r.github.io/ml/ch1_1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch1_1/</guid>
      <description>&lt;p&gt;Just as we judge tomorrow&amp;rsquo;s weather based on past experience, eaters want to pick a good melon from their past experience, so can a computer help humans to make that happen? If there is such a discipline, where human &amp;ldquo;experience&amp;rdquo; corresponds to the &amp;ldquo;data&amp;rdquo; in the computer, and the computer learns this empirical data to generate an algorithmic model that allows the computer to make valid judgments in the face of new situations, and that is Machine Learning.&lt;/p&gt;
&lt;p&gt;Mitchell, author of another classic textbook, gives a formal definition that assumes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;P&lt;/code&gt;: The performance of a computer program on a task class $T$.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;T&lt;/code&gt;: The type of task the computer program wants to achieve.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;E&lt;/code&gt;: Denotes experience, i.e., a historical data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the computer program obtained an improvement in performance $P$ on task $T$ by using experience $E$, the program is said to have learned from $E$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>https://p0st3r.github.io/ml/ch1_2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch1_2/</guid>
      <description>&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;Suppose we collect data on a batch of watermelons, e.g.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;(color=green; rootstock=crumpled; knock=muddy)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(color=black; rootstock=slightly crumpled; knock=dull)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(color=light from; rootstock=hard; knock=crisp)&amp;hellip;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each pair of brackets is a record of a watermelon. Definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Data Set&lt;/code&gt;: The collection of all records is &lt;em&gt;data set&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Sample&lt;/code&gt;: Each record is an &lt;em&gt;instance&lt;/em&gt; or &lt;em&gt;sample&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Feature&lt;/code&gt; or &lt;code&gt;Attribute&lt;/code&gt;: A single feature is: a &lt;em&gt;feature&lt;/em&gt; or &lt;em&gt;attribute&lt;/em&gt;. e.g. color or percussion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Vector&lt;/code&gt;: For each record represented on the axis can be represented by a &lt;em&gt;vector&lt;/em&gt;. e.g. (green, huddled, turbid), i.e. each watermelon is: a feature vector.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Dimensionality&lt;/code&gt;: The number of characteristics of a sample is &lt;em&gt;dimensionality&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Dimensional Disaster&lt;/code&gt;: The watermelon&amp;rsquo;s example dimension is 3, when the dimensionality is very large and it called &lt;em&gt;dimensional disaster&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-set&#34;&gt;Data Set&lt;/h2&gt;
&lt;p&gt;When a computer program learns empirical data to generate an algorithm model, each record is called a &lt;code&gt;training sample&lt;/code&gt;, and when the model is trained and we want to test the model&amp;rsquo;s performance with new samples, each new sample is called a &lt;code&gt;test sample&lt;/code&gt;. Definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Training Set&lt;/code&gt;: The set of all training samples is &lt;em&gt;training set&lt;/em&gt;, [special].&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Test Set&lt;/code&gt;: The set of all test samples is &lt;em&gt;test set&lt;/em&gt; , [general].&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Generalization&lt;/code&gt;: The ability of the machine-learned model to apply to the new sample is &lt;em&gt;generalization&lt;/em&gt;. i.e. from special to general.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;classfication&#34;&gt;Classfication&lt;/h2&gt;
&lt;p&gt;In the case of the watermelon, we want the computer to train a decision model to determine whether a new watermelon is a good watermelon or not by learning data about its characteristics. What we can tell is: whether watermelon is good or bad, which is a discrete value. Likewise, there are projections of future population numbers by using population data from previous years, which are continuous values. Definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Classfication&lt;/code&gt;: The problem where the predicted values are discrete is &lt;em&gt;classification&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Regression&lt;/code&gt;: The problem where the predicted values are continuous is &lt;em&gt;regression&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;method-of-learning&#34;&gt;Method of learning&lt;/h2&gt;
&lt;p&gt;In our process of predicting, it is clear that we already know in advance whether the melon in the training set is a good or bad,  the learner learns the characteristics of these melons and thus concludes the law. The watermelon in the training set have been marked, called marking information.&lt;/p&gt;
&lt;p&gt;But there are also cases where the information is not marked. For example, we want to divide a pile of watermelons into two small piles according to their characteristics,  so that the watermelons in a pile are as similar as possible. For this problem, we do not know beforehand how good or bad the watermelons are, the samples are not marked with information. Definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Supervised Learning&lt;/code&gt;: The learning task for which the training data has tagged information is &lt;em&gt;supervised learning&lt;/em&gt;, and it is easy to know that both the classification and regression described above are supervised learning categories.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Unsupervised Learing&lt;/code&gt;: The learning tasks for which the training data is not labeled with information are &lt;em&gt;unsupervised learning&lt;/em&gt;, commonly known as clustering and association rules.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Error and overfitting</title>
      <link>https://p0st3r.github.io/ml/ch2_1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch2_1/</guid>
      <description>&lt;h2 id=&#34;error&#34;&gt;Error&lt;/h2&gt;
&lt;p&gt;The difference between the learner&amp;rsquo;s actual prediction of the sample and the true value of the sample is called &lt;code&gt;error&lt;/code&gt;. Definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Training Error&lt;/code&gt; or &lt;code&gt;Empirical Error&lt;/code&gt;: Error in the training set&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Test Error&lt;/code&gt;: Error in the test set&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;`Generalization Error: The learner&amp;rsquo;s error in all new samples&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;overfitting&#34;&gt;Overfitting&lt;/h2&gt;
&lt;p&gt;Apparently, we want learners perform well on the new sample which with small generalization errors. Therefore, the learners should be able to learn as many universal &lt;strong&gt;general characterisitics&lt;/strong&gt; from the training set as possible, so as to make the correct discrimination when encountering new samples.&lt;/p&gt;
&lt;p&gt;However, when learners learn the traing set &lt;strong&gt;too wel&lt;/strong&gt; that take some of the training sample&amp;rsquo;s own characteristics as a general feature; there are also cases where the learning capacity is insufficient to learn the basic characteristics of the training set. Definitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Overfitting&lt;/code&gt;: Over-learning to the point of learning the not-so-generic characteristics included in the training sample&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Underfitting&lt;/code&gt;: The learning ability is so poor that the general properties of the training sample have not been learned well&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is known that in the overfitting problem, the training error is very small, but the test error is large; in the underfitting problem, both the training error and the test error are large. Currently, the underfitting problem is relatively easy to overcome, such as increasing the number of iterations, but there is still no very good solution to the overfitting problem, and overfitting is a key obstacle to machine learning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/3c5a03bf3c29a9c1926ce013d3ac938e93d87ac9/68747470733a2f2f692e6c6f6c692e6e65742f323031382f31302f31372f356263373138313137323939362e706e67&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Method of Evaluation</title>
      <link>https://p0st3r.github.io/ml/ch2_2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch2_2/</guid>
      <description>&lt;p&gt;In realistic tasks, we often have multiple algorithms to choose from, so how do chonse the best one for us? As mentioned in last chapter, we want the learner with the &amp;lsquo;smallest generalization error&amp;rsquo;, and the ideal solution is to evaluate the generalization error of the model and select the smallest one. However, the generalization error refers to the ability of the model to be applied to all new samples that we do not have direct access to the it.&lt;/p&gt;
&lt;p&gt;Thus, we usually use a &amp;lsquo;test set&amp;rsquo; to test the learner&amp;rsquo;s ability to discriminate on new samples, and then use the &lt;strong&gt;test error&lt;/strong&gt; on the &lt;strong&gt;test set&lt;/strong&gt; as an approximation of the &lt;strong&gt;generalization error&lt;/strong&gt;. Obviously the test set which we select should be as mutually exclusive as possible with the training set, and here&amp;rsquo;s a little story to explain why.&lt;/p&gt;
&lt;p&gt;Suppose the teacher has 10 questions for the students to practice, and uses the same 10 questions for the test, however some children may could only do these 10 questions and get a high score. It is clear that the score does not reflect the real level effectively.&lt;/p&gt;
&lt;p&gt;In our task, we would like to have a well generalized models, as the teacher would like the students not only learned the course well but also gained the ability to think about what they have learned.&lt;/p&gt;
&lt;p&gt;Training samples are equivalent to the exercises for students to practice, and the testing process is equivalent to an exam. If the test sample had been used for training, it would have been an over-optimistic estimate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Split Traing set and Test set</title>
      <link>https://p0st3r.github.io/ml/ch2_3/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch2_3/</guid>
      <description>&lt;p&gt;In order to use the &lt;strong&gt;test error&lt;/strong&gt; of a &lt;strong&gt;test set&lt;/strong&gt; as an approximation of the &lt;strong&gt;generalization error&lt;/strong&gt;, we need to effectively split the initial data set into mutually exclusive &lt;strong&gt;training sets&lt;/strong&gt; and &lt;strong&gt;test sets&lt;/strong&gt;. The following are some common methods.&lt;/p&gt;
&lt;h2 id=&#34;hold-out&#34;&gt;Hold-out&lt;/h2&gt;
&lt;p&gt;Divide the data set $D$ into two mutually exclusive sets, one as the training set $S$ and one as the test set $T$, satisfying $D=S{\cup}T$ and $S{\cap}T=\phi$. The common division is about 2/3-4/5 samples are used for training and the rest for testing.&lt;/p&gt;
&lt;p&gt;It is notable that the division of the training/test sets should be as consistent as possible in the distribution of the data to avoid additional bias, the stratification is commonly used to sovle this problem.&lt;/p&gt;
&lt;p&gt;At the same time, the results of the single hold-out are often not stable enough due to the random nature of the division, and generally we take the average of a number of random division repeated experiments.&lt;/p&gt;
&lt;h2 id=&#34;cross-validation&#34;&gt;Cross Validation&lt;/h2&gt;
&lt;p&gt;Divide the data set $D$ into $k$ mutually exclusive subsets of equal size, satisfying $D=D_1{\cup}D_2{\cup}&amp;hellip; {\cup}D_k$, $D_i{\cap}D_j=\phi (i{\neq}j)$, similarly using stratification to obtain these subsets that keeping the data distribution as consistent as possible.&lt;/p&gt;
&lt;p&gt;The idea of the cross-validation method is that each time a sum of $k-1$ subsets is used as the training set and the remaining is used as the test set, so as to obtain $k$ cases of training/test set division to do $k$ training &amp;amp;testing, and return the mean of the $k$ test results eventually.&lt;/p&gt;
&lt;h3 id=&#34;k-fold-cross-validation&#34;&gt;K-fold Cross Validation&lt;/h3&gt;
&lt;p&gt;Cross-validation is also called &lt;code&gt;K-fold Cross Validation&lt;/code&gt;, the most common value of $k$ is 10. The following gives a diagram of 10-fold cross-validation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/bc0633094f7c687bb591366fc0a0a65fb489b789/68747470733a2f2f692e6c6f6c692e6e65742f323031382f31302f31372f356263373138313135643232342e706e67&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;leave-one-out&#34;&gt;Leave-One-Out&lt;/h3&gt;
&lt;p&gt;Similar to the hold-out, the data set $D$ is divided into $k$ subsets at random. Therefore K-fold Cross Validation is usually repeated $p$ times as p-times k-fold Cross Validation, which is commonly 10-times 10-fold Cross Validation that performe 100 training/testing sessions.&lt;/p&gt;
&lt;p&gt;In particular, when there is only one sample in each subsets of divied $k$ subsets, it is known as the &lt;code&gt;Leave-One-Out&lt;/code&gt;. The results of the Leave-One-Out are more accurate, but with significant computer consumption.&lt;/p&gt;
&lt;h2 id=&#34;bootstrapping&#34;&gt;Bootstrapping&lt;/h2&gt;
&lt;p&gt;What we want to evaluate is the model that was trained with the whole $D$. However, in the Hold-out and Cross Validation, the actual evaluated model uses a smaller training set than $D$ because a portion of the sample is retained for testing, which inevitably introduces some estimation biases due to differences in training sample size. The Leave-One-Out is less affected by changes in training sample size, but the computational complexity is too high. The Bootstrapping solves precisely that problem.&lt;/p&gt;
&lt;p&gt;The basic idea of the Bootstrapping is given a dataset $D$ containing $m$ samples, randomly selected from $D$ one sample at a time copied into $D&#39;$, and then put it back into the initial dataset $D$ to be picked up at the next sampling. Repeating $m$ times to obtain a dataset $D&#39;$ containing $m$ samples.&lt;/p&gt;
&lt;p&gt;It can be known that the limit of the probability that the sample remain uncollected in $m$ times of sampling is:&lt;/p&gt;
&lt;p&gt;${\lim\limits_{m\to\infty}}{(1-\frac{1}{m})^m\to\frac{1}{e}\approx0.368}$&lt;/p&gt;
&lt;p&gt;Thus, approximately 36.8% of the initial sample set $D$ did not appear in $D&#39;$ through bootstrapping sampling, so $D&#39;$ could be used as the training set and $D-D&#39;$ as the test set. The Bootstrapping is useful when the data set is small which is difficult to spilt the training/test set effectively, however it introduces estimation bias because the data set generated by the bootstraping (random sampling) alters the distribution of the initial data set. When the initial data set is sufficient, Hold-out and Cross Validation are more commonly used.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parameter Tuning</title>
      <link>https://p0st3r.github.io/ml/ch2_4/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch2_4/</guid>
      <description>&lt;p&gt;Most learning algorithms have some parameters that need to be set, which is commonly referred to as &lt;code&gt;parameter tuning&lt;/code&gt;, and the performance of the learned model often varies significantly depending on the parameter configuration.&lt;/p&gt;
&lt;p&gt;Many parameters of the learning algorithm are taken in the real range, so it is not feasible to train a model for each parameter. It is common to select a range and step $\lambda$ for each parameter, which makes the learning process feasible.&lt;/p&gt;
&lt;p&gt;For example, assuming that the algorithm has 3 parameters, each considering only 5 candidate values, there are $5^3$ = $125$ models to examine for each training/test set.&lt;/p&gt;
&lt;p&gt;It is notable that once the model and paramters have been set, we need to retrain the model using the initial dataset $D$. This means that the test set initially divided for evaluation is also learned by the model to enhance the learning effct.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Performance Measure</title>
      <link>https://p0st3r.github.io/ml/ch2_5/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch2_5/</guid>
      <description>&lt;p&gt;Performance measures are evaluation criteria that measure the ability of models to generalize, and when comparing the ability of different models, different performance measures often results in different judgments.&lt;/p&gt;
&lt;h2 id=&#34;the-most-common-performance-measures&#34;&gt;The Most Common Performance Measures&lt;/h2&gt;
&lt;h3 id=&#34;mean-squared-erro&#34;&gt;Mean Squared Erro&lt;/h3&gt;
&lt;p&gt;In the regression task which predict continuous values, the most commonly used is the &lt;code&gt;Mean Squared Error&lt;/code&gt;, many classical algorithms are using MSE as an evaluation function.&lt;/p&gt;
&lt;p&gt;$E(f;D)=\frac{1}{m}\sum\limits_{i=1}^{m}(f(x_i)-y_i)^2$&lt;/p&gt;
&lt;p&gt;More generally, for data distribution $\mathcal{D}$ and probability density functions $p(\cdot)$, the MSE can be described as&lt;/p&gt;
&lt;p&gt;$E(f;\mathcal{D})=\int_{x\sim\mathcal{D}}(f(x)-y)^2p(x)dx$&lt;/p&gt;
&lt;h3 id=&#34;error-rate--precision&#34;&gt;Error rate &amp;amp; Precision&lt;/h3&gt;
&lt;p&gt;In the classification task which predict discrete values, the most commonly used are error rate and precision, where error rate is the number of samples classified incorrectly as a proportion of the total number of samples, and precision is the number of correctly classified samples as a proportion of the total number of samples, easily known that error rate + precision = 1.&lt;/p&gt;
&lt;p&gt;Error rate is defined as:&lt;/p&gt;
&lt;p&gt;$E(f;D)=\frac{1}{m}\sum\limits_{i=1}^{m}\mathbb I(f(x_i)\neq{y_i})$&lt;/p&gt;
&lt;p&gt;Precision is defined as:&lt;/p&gt;
&lt;p&gt;$\begin{align*}acc(f;D)&amp;amp;=\frac{1}{m}\sum\limits_{i=1}^{m}\mathbb I(f(x_i)=y_i)=1-E(f;D)\end{align*}$&lt;/p&gt;
&lt;p&gt;More generally, for data distribution $\mathcal{D}$ and probability density functions $p(\cdot)$, the Error Rate and Precision can be described as:&lt;/p&gt;
&lt;p&gt;$E(f;\mathcal{D})=\int_{x\sim\mathcal{D}}\mathbb I(f(x)\neq{y})p(x)dx$&lt;/p&gt;
&lt;p&gt;$acc(f;\mathcal{D})=\int_{x\sim\mathcal{D}}\mathbb I(f(x)=y)p(x)dx=1-E(f;\mathcal{D})$&lt;/p&gt;
&lt;h2 id=&#34;accuracyrecallf1&#34;&gt;Accuracy/Recall/F1&lt;/h2&gt;
&lt;p&gt;For example, in a recommendation system, we only care about whether the content pushed to the user is of interest to the user (i.e. accuracy), or how much of all the content of interest to the user we pushed (i.e. recall). Therefore, the search accuracy/recall is more appropriate to describe such issues. For the binary classification, the classification result confusion matrix and the accuracy/racall are defined as follows:&lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
		&lt;td rowspan=&#34;2&#34;&gt;Fact&lt;/td&gt;
        &lt;td colspan=&#34;2&#34; &gt;Prediction&lt;/td&gt;
        &lt;tr&gt;
        	&lt;td&gt;Positive&lt;/td&gt;
			&lt;td&gt;Negative&lt;/td&gt;
    	&lt;/tr&gt;
     &lt;/tr&gt;
	&lt;/tr&gt;
		&lt;td&gt;Positive&lt;/td&gt;
		&lt;td&gt;TP(Ture Positive)&lt;/td&gt;
    	&lt;td&gt;FN(False Negative)&lt;/td&gt;
	&lt;tr&gt;
    	&lt;td&gt;Negative&lt;/td&gt;
        &lt;td&gt;FP(False Positive)&lt;/td&gt;
		&lt;td&gt;TN(Ture Negative)&lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Accuracy $P$ and Recall $R$ are defined as:&lt;/p&gt;
&lt;p&gt;$P=\frac{TP}{TP+FP}$&lt;/p&gt;
&lt;p&gt;$R=\frac{TP}{TP+FN}$&lt;/p&gt;
&lt;p&gt;Accuracy and Recall are a pair of contradictory measures. For example, if we want content pushed to be as interesting as possible to all users, we can only push the content which is certainly so that some content users are interestd will be missed, leads to low racall; if we want all the content which users are interested pushed, we only push all the content so that accuracy is very low.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;P-R curve&lt;/code&gt; is precisely the curve describing the change of the accuracy/racall, the P-R curve is defined as follows: according to the prediction result of the learner (generally a real value or probability), the test samples are ranked, the samples most likely to be the &lt;em&gt;positive example&lt;/em&gt; in the front, the least likely to be the &lt;em&gt;positive example&lt;/em&gt; in the back, and the samples are predicted as the &lt;em&gt;positive example&lt;/em&gt; one by one in this order, and the current $P$ and $R$ values are calculated each time, as shown in the figure below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/89c709b82bf20efde039fe32fff5724ec6bbe6d9/68747470733a2f2f692e6c6f6c692e6e65742f323031382f31302f31372f356263373164616663343431312e706e67&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;How is the P-R curve evaluated? If the P-R curve of one learner $A$ is completely covered by the P-R curve of another learner $B$, then the performance of $B$ is better than that of $A$. If the curves of $A$ and $B$ intersect, then who has a larger area under the curve and whose performance is better. But in general, the area under the curve is difficult to estimate, so the &amp;ldquo;Break-Event Point&amp;rdquo; (BEP) is derived, i.e. when P=R, the higher the value of the Break-Event Point, the better the performance.&lt;/p&gt;
&lt;p&gt;The P and R indicators are sometimes contradictory, so they need to be considered together, and the most common method is F-Measure, also known as &lt;code&gt;F-Score&lt;/code&gt;, which is a weighted reconciliation average of $P$ and $R$. i.e.&lt;/p&gt;
&lt;p&gt;$\frac{1}{F_\beta}=\frac{1}{1+\beta^2}\cdot(\frac{1}{P}+\frac{\beta^2}{R})$&lt;/p&gt;
&lt;p&gt;$F_\beta=\frac{(1+\beta^2)\times{P}\times{R}}{(\beta^2\times{P})+R}$&lt;/p&gt;
&lt;p&gt;n particular, when $\beta=1$ it becomes the common $F1$ measure, is a reconciled average of $P$ and $R$, the better the model performs when $F1$ is higher.&lt;/p&gt;
&lt;p&gt;$\frac{1}{F1}=\frac{1}{2}\cdot(\frac{1}{P}+\frac{1}{R})$&lt;/p&gt;
&lt;p&gt;$F1=\frac{2\times{P}\times{R}}{P+R}=\frac{2\times{TP}}{ALL+TP-TN}$&lt;/p&gt;
&lt;p&gt;Sometimes we have multiple bicategorical confusion matrices, e.g., multiple trainings or training on multiple datasets, then there are two ways to estimate global performance, which are macro and micro. Macro is to calculate the $P$ and $R$ values of each confusion matrix first, then obtain the average P value $macroP$ and the average R value $macroR$ to calculating $F\beta$ or $F1$, while micro is to calculate the average TP, FP, TN, FN of the confusion matrix, then calculate $P$, $R$, and thus $F\beta$ or $F1$.&lt;/p&gt;
&lt;p&gt;$macroP=\frac{1}{n}\sum\limits_{i=1}^{n}P_i$&lt;/p&gt;
&lt;p&gt;$macroR=\frac{1}{n}\sum\limits_{i=1}{n}R_i$&lt;/p&gt;
&lt;p&gt;$macro-F1=\frac{2\times{macroP}\times{macroR}}{macroP+macroR}$&lt;/p&gt;
&lt;p&gt;$microP=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$&lt;/p&gt;
&lt;p&gt;$microF1=\frac{2\times{microP}\times{microR}}{microP+microR}$&lt;/p&gt;
&lt;h2 id=&#34;roc--auc&#34;&gt;ROC &amp;amp; AUC&lt;/h2&gt;
&lt;p&gt;The ROC curve is very similar to the P-R curve, both are predicted according to the positive cases one by one according to the order of the order, the difference is that the ROC curve takes the &amp;ldquo;True Positive Rate&amp;rdquo; (TPR) as the horizontal axis and the vertical axis as the &amp;ldquo;False Positive Rate&amp;rdquo; (FPR), the ROC focuses on studying the order of the test sample based on the evaluation value.&lt;/p&gt;
&lt;p&gt;$TPR=\frac{TP}{TP+FN}$&lt;/p&gt;
&lt;p&gt;$FPR=\frac{FP}{TN+FP}$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/fd910cedcf687728efd0d67d938504c5ee7f3cf3/68747470733a2f2f692e6c6f6c692e6e65742f323031382f31302f31372f356263373165643735636566652e706e67&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;A simple analysis of the image shows that when FN=0, TN must also be 0, and vice versa. We can draw a queue and try to split the queue using different truncation points (i.e. thresholds) to analyze the shape of the curve, (0,0) means that all samples are predicted as negative cases, (1,1) means that all samples are predicted as positive cases, (0,1) means the ideal case that all positive cases appear before negative cases, (1,0) means the worst case that all negative cases appear before positive cases.&lt;/p&gt;
&lt;p&gt;Similarly, if the ROC curve of one learner $A$ is completely covered by the other learner $B$, the performance of $B$ is said to superior to that of $A$. If the curves of A and B intersect, then whose curve has more area under it and whose performance is better. The Area under the ROC Curve is defined as AUC (Area Uder ROC Curve). Different from P-R, the AUC here is estimable,i.e. the sum of the Area of each small rectangle under the AOC Curve. It&amp;rsquo;s easy to see that the larger the AUC is, the better the quality of the sort. When the AUC is 1, it mean that all the positive examples are in front of the negative ones, and when the AUC is 0 means all the negative examples are in front of the positive ones.&lt;/p&gt;
&lt;p&gt;$AUC=\frac{1}{2}\sum\limits_{i=1}^{m-1}(x_{i+1}-x_i)\cdot(y_i+y_{i+1})$&lt;/p&gt;
&lt;h2 id=&#34;cost-sensitive-error-rate--cost-curve&#34;&gt;Cost-sensitive Error Rate &amp;amp; Cost Curve&lt;/h2&gt;
&lt;p&gt;In the above approach, the mistakes of the learner are treated equally, but in reality the cost of predicting positive samples into negative is often not the same as predicting negative samples into positive, e.g., predicting no disease &amp;ndash;&amp;gt; having disease just increases the number of checks, but having disease &amp;ndash;&amp;gt; no disease increases the risk to life. Take the binary classification for example, which thus introduces a &lt;code&gt;cost matrix&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
    &lt;tr&gt;
		&lt;td rowspan=&#34;2&#34;&gt;Fact&lt;/td&gt;
        &lt;td colspan=&#34;2&#34; &gt;Prediction&lt;/td&gt;
        &lt;tr&gt;
        	&lt;td&gt;class 0&lt;/td&gt;
			&lt;td&gt;class 1&lt;/td&gt;
    	&lt;/tr&gt;
     &lt;/tr&gt;
	&lt;/tr&gt;
		&lt;td&gt;class 0&lt;/td&gt;
		&lt;td&gt;0&lt;/td&gt;
    	&lt;td&gt;$cost_{10}$&lt;/td&gt;
	&lt;tr&gt;
    	&lt;td&gt;class 1&lt;/td&gt;
        &lt;td&gt;$cost_{01}$&lt;/td&gt;
		&lt;td&gt;0&lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Under non-equal error costs, we want to minimize the &lt;code&gt;overall cost&lt;/code&gt; so that the &lt;code&gt;cost-sensitive&lt;/code&gt; error rate is:&lt;/p&gt;
&lt;p&gt;$E(f;D;cost)=\frac{1}{m}(\sum\limits_{x_i\in{D}^+}\mathbb I(f(x_i)\neq{y_i})\times{cost_{01}}+\sum\limits_{x_i\in{D^-}}{\mathbb I}(f(x_i)\neq{y_i})\times{cost_{10}})$&lt;/p&gt;
&lt;p&gt;Similarly, for the ROC curve, it evolves into a &lt;em&gt;cost curve&lt;/em&gt; at non-equal error costs, where the horizontal axis of the cost curve is the probability cost of taking the positive case between [0,1], where $p$ is the probability of taking the positive case, and the vertical axis is the normalized cost of taking the value [0,1].&lt;/p&gt;
&lt;p&gt;$P(+)cost=\frac{p\times{cost_{01}}}{p\times{cost_{01}}+(1-p)\times{cost_{10}}}$&lt;/p&gt;
&lt;p&gt;$cost_{norm}=\frac{FNR\times{p}\times{cost_{01}}+FPR\times(1-p)\times{cost_{10}}}{p\times{cost_{01}}+(1-p)\times{cost_{10}}}$&lt;/p&gt;
&lt;p&gt;Plot the cost curve is simple: set the coordinates of a point on the ROC curve as (TPR, FPR) to calculated FNR, then plot a line segment from (0, FPR) to (1, FNR) in the cost plane, the area under the line segment represents the desired overall cost under that condition, so transform each point of the ROC curve earth into a line segment in the cost plane, and take the lower boundary of all line segments, the enclosed area is the desired overall cost of the learner under all conditions, as shown in the figure:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://camo.githubusercontent.com/181fd50db67c159c2e84a1a45c8d8a8e7480cc5c/68747470733a2f2f692e6c6f6c692e6e65742f323031382f31302f31372f356263373165643731366530642e706e67&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear regression</title>
      <link>https://p0st3r.github.io/ml/ch3_1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch3_1/</guid>
      <description>&lt;p&gt;lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear probability regression</title>
      <link>https://p0st3r.github.io/ml/ch3_2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch3_2/</guid>
      <description>&lt;p&gt;lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic concepts of Decision tree</title>
      <link>https://p0st3r.github.io/ml/ch4_1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/ml/ch4_1/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://p0st3r.github.io/ml/ch2_6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://p0st3r.github.io/ml/ch2_6/</guid>
      <description>&lt;p&gt;In the last two articles, we introduced a variety of common evaluation methods and performance measures so that we can select the most appropriate one to calculate the learner&amp;rsquo;s &lt;em&gt;test error&lt;/em&gt; based on the characteristics of the data set and model task.&lt;/p&gt;
&lt;p&gt;Howerver, the &lt;em&gt;test error&lt;/em&gt; is affected by many factors such as the randomness of the algorithm (e.g. K-Means) and the diffirence between test sets, which makes the same model get different results each time. Also the &lt;em&gt;test error&lt;/em&gt; is an approximation of the &lt;em&gt;generalization error&lt;/em&gt; instead of the true generalization performance of the learners.&lt;/p&gt;
&lt;p&gt;So how to compare the performance measures of single or multiple learners on different/same test set? And that is the comparative test. Final bias and variance is an important tool to explain learner generalization performance. This post continues from the previous post and focuses on comparison tests, variance and bias.&lt;/p&gt;
&lt;h2 id=&#34;comparison-test&#34;&gt;Comparison test&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
