<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>7 | HumblePoster</title>
    <link>https://p0st3r.github.io/publication_types/7/</link>
      <atom:link href="https://p0st3r.github.io/publication_types/7/index.xml" rel="self" type="application/rss+xml" />
    <description>7</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Lithium©2016</copyright><lastBuildDate>Sun, 12 May 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://p0st3r.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>7</title>
      <link>https://p0st3r.github.io/publication_types/7/</link>
    </image>
    
    <item>
      <title>WebShell detection based on semantic features</title>
      <link>https://p0st3r.github.io/publication/webshell-detection-based-on-semantic-features/</link>
      <pubDate>Sun, 12 May 2019 00:00:00 +0000</pubDate>
      <guid>https://p0st3r.github.io/publication/webshell-detection-based-on-semantic-features/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In order to cope with the current complex and flexible WebShell attack environment and better detect various types of webshells, a text classfication method that combains word2vec and Bi-Gram, which not only considered the semantic features but also the word order features, has a better classfication result than using two features alone.&lt;/p&gt;
&lt;p&gt;In order to cope with the current complex and flexible WebShell attack environment and better detect various types of webshells, an improved WebShell detection method based on CNN was proposed by improving text feature extraction and convolutional neural network structure. First, precompile the PHP data set to get the opcode instruction sequence. Secondly, Word2vec is used to extract the word vectors of the original text and the phrase segmented by bi-gram respectively, and it serves as the two inputs of the convolutional neural network. Finally, the detection is carried out through the designed convolutional neural network. Through experiments, this method effectively improves the accuracy, recall rate and other performance parameters while compared with other methods.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;word-ng-vec&#34;&gt;Word-NG vec&lt;/h3&gt;
&lt;p&gt;In fact, the word embedding learned in Word2vec are more reflect in semantic similarity features , such as &amp;ldquo;extract&amp;rdquo; and &amp;ldquo;take&amp;rdquo;, &amp;ldquo;compression&amp;rdquo; and &amp;ldquo;reduction&amp;rdquo;, but without taking the word order features into account. Sometimes there are some differences with the actual semantics, for example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The USA started a trade war on China&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The main predicates of two sentences are interchanged to express different meanings, but there is no difference in Word2Vec. However, if N-Gram is used to divide the text into word groups , we can obtain the word order feature of the text，such as:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Orignal&lt;/th&gt;
&lt;th&gt;The USA started a trade war on China&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Word2vec&lt;/td&gt;
&lt;td&gt;“The USA”，“started”，&amp;ldquo;a&amp;rdquo;，“trade war”，“on”，“China” (vector)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2-gram&lt;/td&gt;
&lt;td&gt;“The USA/The USA” ，“The USA/started”，&amp;ldquo;stared&amp;rdquo;/a”，“a/trade war”，&amp;ldquo;trade war/on&amp;rdquo;，“on China”&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/liyuanzi/WordNG-vec_WebShell_detect/blob/master/assets/1.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://github.com/liyuanzi/WordNG-vec_WebShell_detect/raw/master/assets/1.jpg&#34; alt=&#34;1&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Combining N-Gram with Word2vec, the word vectors of the original text and 2-gram pharse are both trained by Word2vec as the general features of the text, taking into account not only the semantic features, but also the word order features. In this way, two similar sentences can be correctly classifcated.&lt;/p&gt;
&lt;h3 id=&#34;convolutional-neural-network&#34;&gt;Convolutional Neural Network&lt;/h3&gt;
&lt;p&gt;Based on the Kim Y&amp;rsquo;s 
&lt;a href=&#34;https://arxiv.org/abs/1408.5882&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TextCNN&lt;/a&gt;, combined with the two-channel convolutional neural network, a text classification model DCTF-CNN(Double-Channels and Trible-Filters Convolutional Neural Network) was constructed.&lt;/p&gt;
&lt;p&gt;The structure of the neural network is shown below:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/liyuanzi/WordNG-vec_WebShell_detect/blob/master/assets/2.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://github.com/liyuanzi/WordNG-vec_WebShell_detect/raw/master/assets/2.jpg&#34; alt=&#34;2&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The model includes five layers: input layer, convolution layer, pooling layer, full-link layer and output layer.In the input layer, T1 channel and T2 channel respectively input the word vector of the original text and the word vector of the 2-gram pharse. The convolution layer is composed of three convolution kernels of different widths, each of which covers the local characteristics of different granularity.In the pooling layer, the global maximum value is pooled, and the maximum value is reserved for each convolution kernel, which can effectively extract the most representative features.Two feature vectors were spliced in the full link layer, and finally two types of distributions, namely the probability distribution of WebShell and normal files, were output through Softmax in the output layer.&lt;/p&gt;
&lt;h3 id=&#34;php-opcode&#34;&gt;PHP opcode&lt;/h3&gt;
&lt;p&gt;PHP is an interpreted language, and its code execution process can be divided into Lexical Analysis stage, Syntax Analysis stage, byte code compilation stage and code execution stage.The execution flow diagram is shown in the solid line section in the below:&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/liyuanzi/WordNG-vec_WebShell_detect/blob/master/assets/3.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;https://github.com/liyuanzi/WordNG-vec_WebShell_detect/raw/master/assets/3.jpg&#34; alt=&#34;3&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the lexical analysis phase, the Lexer reads the source code sequence of characters in sequence and shred them into Token sequences according to PHP syntax rules.In the Syntax analysis stage, the Token sequence is read in by the Parser to be syntactically checked, and then the Abstract Syntax Tree (AST) is generated.In the bytecode compilation phase, the PHP virtual machine Zend reads in the abstract syntax tree and translates the action nodes in the syntax tree into the corresponding bytecode.In the code execution stage, the PHP virtual machine Zend loads the corresponding module according to the code call, initializes the running environment, and finally executes the bytecode instruction and outputs the result.&lt;/p&gt;
&lt;p&gt;For example, the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;?php
echo’Hello World’;
$a=1+1;
echo $a;
?&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After PHP code compilation by VLD extension, the code can be compiled into following opcode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ZEND_ECHO ’Hello World’
ZEND_ADD ~ 0 1 1
ZEND_ASSIGN!0 ~ 0
ZEND_ECHI ~ 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this paper, the collected PHP datasets is compiled into opcode to word embedding, so as to avoid the interference of useless annotations added in the WebShell which might bypass static detection. In this way the generalization of the model could be imporved&lt;/p&gt;
&lt;h2 id=&#34;assessment&#34;&gt;Assessment&lt;/h2&gt;
&lt;p&gt;Experimental data sets were obtained from the following sources:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;sources&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;WebShell&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/tennc/webshell&#34;&gt;https://github.com/tennc/webshell&lt;/a&gt; &lt;a href=&#34;https://github.com/JohnTroony/php-webshells&#34;&gt;https://github.com/JohnTroony/php-webshells&lt;/a&gt; &lt;a href=&#34;https://github.com/ysrc/webshell-sample&#34;&gt;https://github.com/ysrc/webshell-sample&lt;/a&gt; &lt;a href=&#34;https://github.com/tanjiti/webshellSample&#34;&gt;https://github.com/tanjiti/webshellSample&lt;/a&gt; &lt;a href=&#34;https://github.com/xl7dev/WebShell&#34;&gt;https://github.com/xl7dev/WebShell&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Normal Page&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/WordPress/WordPress&#34;&gt;https://github.com/WordPress/WordPress&lt;/a&gt; &lt;a href=&#34;https://github.com/phpmyadmin/phpmyadmin&#34;&gt;https://github.com/phpmyadmin/phpmyadmin&lt;/a&gt; &lt;a href=&#34;https://github.com/typecho/typecho&#34;&gt;https://github.com/typecho/typecho&lt;/a&gt; &lt;a href=&#34;https://github.com/bcit-ci/CodeIgniter&#34;&gt;https://github.com/bcit-ci/CodeIgniter&lt;/a&gt; &lt;a href=&#34;https://github.com/laravel/laravel&#34;&gt;https://github.com/laravel/laravel&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Remove duplicate files by md5 comparsion, a total of 2387 WebShell samples and 2316 Normal Page samples were obtained.&lt;/p&gt;
&lt;p&gt;The WebShell samples covers One-word Trojan, small Trojan and giant Trojan all types of WebShell.&lt;/p&gt;
&lt;p&gt;The Normal Page covers blog cms，php development framework and database management system, and the similar page was remove to optimize data sets.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;Download code and data sets&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/liyuanzi/WordNG-vec_WebShell_detect
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This code requires Python2.7&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name py27 python=2.7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Initialization environment&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requerments.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;start trainning&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ nohup python -u webshell.py &amp;gt;dctf.txt 2&amp;gt;&amp;amp;1 &amp;amp; tail -f dctf.txt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The word embedding process takes a very long time，use &lt;code&gt;nohup&lt;/code&gt; to avoid the word embedding process fail because the terminal closed unexpectly. The result is loaded into &lt;code&gt;dctf.txt&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p&gt;A Comparsion examination on a small size data sets .&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;features&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;th&gt;Precision&lt;/th&gt;
&lt;th&gt;Recall&lt;/th&gt;
&lt;th&gt;F1-score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2-gram&lt;/td&gt;
&lt;td&gt;0.798&lt;/td&gt;
&lt;td&gt;0.714&lt;/td&gt;
&lt;td&gt;0.196&lt;/td&gt;
&lt;td&gt;0.308&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;opcode sequences&lt;/td&gt;
&lt;td&gt;0.969&lt;/td&gt;
&lt;td&gt;0.998&lt;/td&gt;
&lt;td&gt;0.823&lt;/td&gt;
&lt;td&gt;0.902&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Word2vec&lt;/td&gt;
&lt;td&gt;0.958&lt;/td&gt;
&lt;td&gt;0.994&lt;/td&gt;
&lt;td&gt;0.918&lt;/td&gt;
&lt;td&gt;0.954&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bigram-word2vec&lt;/td&gt;
&lt;td&gt;0.960&lt;/td&gt;
&lt;td&gt;0.978&lt;/td&gt;
&lt;td&gt;0.937&lt;/td&gt;
&lt;td&gt;0.958&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Word-NG vec&lt;/td&gt;
&lt;td&gt;0.984&lt;/td&gt;
&lt;td&gt;0.995&lt;/td&gt;
&lt;td&gt;0.971&lt;/td&gt;
&lt;td&gt;0.982&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
  </channel>
</rss>
