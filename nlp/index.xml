<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing | HumblePoster</title>
    <link>https://p0st3r.github.io/nlp/</link>
      <atom:link href="https://p0st3r.github.io/nlp/index.xml" rel="self" type="application/rss+xml" />
    <description>Natural Language Processing</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Lithium©2016</copyright><lastBuildDate>Sun, 09 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://p0st3r.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Natural Language Processing</title>
      <link>https://p0st3r.github.io/nlp/</link>
    </image>
    
    <item>
      <title>Named Entity Recognition</title>
      <link>https://p0st3r.github.io/nlp/ner/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://p0st3r.github.io/nlp/ner/</guid>
      <description>&lt;p&gt;Source：
&lt;a href=&#34;https://arxiv.org/pdf/1812.09449.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Survey on Deep Learning for Named Entity Recognition&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Named entity recognition (NER) is the task to identify text spans that mention named entities, and to classify them into predefined categories such as person, location, organization etc. NER serves as the basis for a variety of natural language applications such as question answering, text summarization, and machine translation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;命名实体识别(NER)旨在从文本中识别出特殊对象，这些对象的语义类别通常在识别前被预定义好，预定义类别如人、地址、组织等。命名实体识别不仅仅是独立的信息抽取任务，它在许多大型自然语言处理应用系统如信息检索、自动文本概要、问答任务、机器翻译以及知识建库（知识图谱）中也扮演了关键的角色。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This paper introduced four aspects of the NER:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Introduced NER resources, including tagged NER corpora and off-the-shelf NER tools.&lt;/li&gt;
&lt;li&gt;Systematically categorized existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder.&lt;/li&gt;
&lt;li&gt;The most representative methods for recent applied techniques of deep learning in new NER problem settings and applications.&lt;/li&gt;
&lt;li&gt;The challenges faced by NER systems and outline future directions in this area.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;这篇文章主要介绍了NER的四个方面&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;介绍了NER的资源，包括标签化的NER词库和现成的NER工具&lt;/li&gt;
&lt;li&gt;基于分类法从三个角度对现有的工作进行系统的分类：输入的分布式表示，内容编码器，标签解码器&lt;/li&gt;
&lt;li&gt;深度学习的最新应用技术在新的NER问题设置和应用中的最具代表性的方法&lt;/li&gt;
&lt;li&gt;NER系统所面临的挑战以及在这一领域的未来发展方向。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;named-entity&#34;&gt;Named Entity&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The author divides named entities into two categories：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generic named entities，such as people and places.&lt;/li&gt;
&lt;li&gt;specific domain named entities，such as proteins，genes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This paper focuses on the first type of named entity recognition task in English.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者讲命名实体分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;常见的命名实体，如人名和地名&lt;/li&gt;
&lt;li&gt;特殊领域的命名实体，如蛋白质、基因&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇文章主要聚焦于英文的第一类命名实体识别的任务。&lt;/p&gt;
&lt;h4 id=&#34;methods-for-ner&#34;&gt;Methods for NER&lt;/h4&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Rule-based approaches, which do not need annotated data as they rely on hand-crafted rules&lt;/li&gt;
&lt;li&gt;Unsupervised learning approaches, which rely on un-supervised algorithms without hand-labeled training examples&lt;/li&gt;
&lt;li&gt;Feature-based supervised learning approaches, which rely on supervised learning algorithms with careful feature engineering&lt;/li&gt;
&lt;li&gt;Deep-learning based approaches, which automatically discover representations needed for the classification and/or detection from raw input in an end-to-end manner.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;基于规则的方法&lt;/li&gt;
&lt;li&gt;无监督方法&lt;/li&gt;
&lt;li&gt;基于特征的监督学习方法&lt;/li&gt;
&lt;li&gt;深度学习方法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上述分类法并非泾渭分明的，比如某些深度学习方法也结合了一些研究者设计的特征来提高识别的准确率。&lt;/p&gt;
&lt;h3 id=&#34;formalize-definition&#34;&gt;Formalize Definition&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a sequence of tokens s=&amp;lt;w1,w2,…,wn&amp;gt;s=&amp;lt;w1,w2,…,wn&amp;gt;, NER is to output a list of tuples &amp;lt;Is,Ie,t&amp;gt;&amp;lt;Is,Ie,t&amp;gt;, each of which is a named entity mentioned in ss. Here, Is∈[1,N]Is∈[1,N] and Ie∈[1,N]Ie∈[1,N] are the start and the end indexes of a named entity mention; tt is the entity type from a predefined category set. Example as:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;assets/image-20200418111239312.png&#34; alt=&#34;image-20200418111239312&#34;&gt;&lt;/p&gt;
&lt;p&gt;给定标识符集合 s=&amp;lt;w1,w2,…,wn&amp;gt;s=&amp;lt;w1,w2,…,wn&amp;gt; ，NER 输出一个三元组的 &amp;lt;Is,Ie,t&amp;gt;&amp;lt;Is,Ie,t&amp;gt; 列表，列表中的每个三元组代表 ss 中的一个命名实体。此处 Is∈[1,N]Is∈[1,N]，Ie∈[1,N]Ie∈[1,N] 分别为命名实体的起始索引以及结束索引；tt 指代从预定义类别中选择的实体类型&lt;/p&gt;
&lt;h2 id=&#34;tasks-of-ner&#34;&gt;Tasks of NER&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Coarse-grained NER: focuses on a small set of coarse entity types and one type per named entity&lt;/li&gt;
&lt;li&gt;Fine-grained NER: focus on a much larger set of entity types where a mention may be assigned multiple types&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NOTE: The change of NER task from coarse to fine is closely related to the development of annotated data sets from small to large&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;粗粒度的NER（实体种类少，每个命名实体对应一个实体类型）&lt;/li&gt;
&lt;li&gt;细粒度的NER（实体种类多，每个命名实体可能存在多个对应的实体类型）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;值得一提的是，NER任务从粗到细的变化与标注数据集从小到大的发展密切相关&lt;/p&gt;
&lt;h2 id=&#34;ner-resources&#34;&gt;NER Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;High quality annotation are critical for both model learning and evaluation. Before 2005, datasets were mainly developed by annotating news articles with a small number of entity types, suitable for coarse-grained NER tasks. After that, more datasets were developed on various kinds of text sources including Wikipedia articles, conversation, and user-generated text(e.g., tweets and YouTube comments and StackExchange posts in W-NUT). The number of tag types becomes significantly larger, e.g., 89 in OntoNotes.&lt;/p&gt;
&lt;p&gt;Note that many recent NER works report their perfor-mance on CoNLL03 and OntoNotes datasets, they are used for coarse-grained and fine-grained NER tasks, respectively.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有监督方法的NER任务依赖标注数据集。2005 年之前，数据集主要通过标注新闻文章得到并且预定义的实体种类少，这些数据集适合用于粗粒度的NER任务; 2005 年之后，数据集来源越来越多，包括但不限于维基百科文章、对话、用户生成语料（如推特等社区的用户留言）等，并且预定义的实体类别也多了许多，以数据集 OneNotes 为例，其预定义的实体类别达到了89种之多。&lt;/p&gt;
&lt;p&gt;所有数据集中，最常见的数据集为 CoNLL03 和 OneNotes，分别常见于粗粒度的NER任务和细粒度的NER任务。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Corpus&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Year&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Text Source&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;#Tags&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;URL&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MUC-6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Wall Street Journal texts&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2003T13&#34;&gt;https://catalog.ldc.upenn.edu/LDC2003T13&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MUC-6 Plus&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Additional news to MUC-6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC96T10&#34;&gt;https://catalog.ldc.upenn.edu/LDC96T10&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;MUC-7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1997&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;New York Times news&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2001T02&#34;&gt;https://catalog.ldc.upenn.edu/LDC2001T02&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;CoNLL03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2003&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Reuters news&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.clips.uantwerpen.be/conll2003/ner/&#34;&gt;https://www.clips.uantwerpen.be/conll2003/ner/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;ACE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2000-2008&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Transcripts, news&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.ldc.upenn.edu/collaborations/past-projects/ace&#34;&gt;https://www.ldc.upenn.edu/collaborations/past-projects/ace&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OntoNotes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2007-2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Magazine, news, conversation, web&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;89&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2013T19&#34;&gt;https://catalog.ldc.upenn.edu/LDC2013T19&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;W-NUT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015-2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;User-generated text&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://noisy-text.github.io/&#34;&gt;http://noisy-text.github.io/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BBN&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2005&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Wall Street Journal texts&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/ldc2005t33&#34;&gt;https://catalog.ldc.upenn.edu/ldc2005t33&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;NYT&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2008&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;New Yorks Times texts&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC2008T19&#34;&gt;https://catalog.ldc.upenn.edu/LDC2008T19&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WikiGold&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2009&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Wikipedia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/546250&#34;&gt;https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/546250&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WiNER&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Wikipedia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner&#34;&gt;http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;WikiFiger&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2012&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Wikipedia&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;113&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/xiaoling/figer&#34;&gt;https://github.com/xiaoling/figer&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;N3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2014&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://aksw.org/Projects/N3NERNEDNIF.html&#34;&gt;http://aksw.org/Projects/N3NERNEDNIF.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;GENIA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2004&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Biology and clinical texts&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.geniaproject.org/home&#34;&gt;http://www.geniaproject.org/home&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;GENETAG&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2005&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MEDLINE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://sourceforge.net/projects/bioc/files/&#34;&gt;https://sourceforge.net/projects/bioc/files/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;FSU-PRGE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2010&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PubMed and MEDLINE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://julielab.de/Resources/FSU_PRGE.html&#34;&gt;https://julielab.de/Resources/FSU_PRGE.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;NCBI-Disease&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2014&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PubMed&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;790&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/&#34;&gt;https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;BC5CDR&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2015&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;PubMed&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://bioc.sourceforge.net/&#34;&gt;http://bioc.sourceforge.net/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;DFKI&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2018&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bussiness news and social media&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://dfki-lt-re-group.bitbucket.io/product-corpus/&#34;&gt;https://dfki-lt-re-group.bitbucket.io/product-corpus/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Tools&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Off-the-shelf NER tools offered by academia and industry/open source projects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;现成的NER工具来源于学界、工业界以及开源项目&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;NER System&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;URL&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;StanfordCoreNLP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://stanfordnlp.github.io/CoreNLP/&#34;&gt;https://stanfordnlp.github.io/CoreNLP/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OSU Twitter NLP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/aritter/twitter_nlp&#34;&gt;https://github.com/aritter/twitter_nlp&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Illinois NLP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://cogcomp.org/page/software/&#34;&gt;http://cogcomp.org/page/software/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;NeuroNER&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://neuroner.com/&#34;&gt;http://neuroner.com/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;NERsuite&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://nersuite.nlplab.org/&#34;&gt;http://nersuite.nlplab.org/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Polyglot&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://polyglot.readthedocs.io/&#34;&gt;https://polyglot.readthedocs.io/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Gimli&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://bioinformatics.ua.pt/gimli&#34;&gt;http://bioinformatics.ua.pt/gimli&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;spaCy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://spacy.io/&#34;&gt;https://spacy.io/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;NLTK&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.nltk.org/&#34;&gt;https://www.nltk.org/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;OpenNLP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://opennlp.apache.org/&#34;&gt;https://opennlp.apache.org/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;LingPipe&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://alias-i.com/lingpipe-3.9.3/&#34;&gt;http://alias-i.com/lingpipe-3.9.3/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;AllenNLP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://allennlp.org/models&#34;&gt;https://allennlp.org/models&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;IBM Watson&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.ibm.com/watson/&#34;&gt;https://www.ibm.com/watson/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;evaluation-metrics&#34;&gt;Evaluation Metrics&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;NER systems are usually evaluated by comparing their outputs against human annotations. The comparison can be quantified by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exact-match&lt;/li&gt;
&lt;li&gt;Relaxed match&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;通常通过与人类标注水平进行比较判断NER系统的优劣。评估分两种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;精确匹配评估&lt;/li&gt;
&lt;li&gt;宽松匹配评估&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;exact-match-evaluation&#34;&gt;Exact-match Evaluation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;NER involves identifying both entity boundaries and entity types. With “exact-match evaluation”, a named entity is considered correctly recognized only if its both boundaries and type match ground truth. Precision, Recall, and F-score are computed on the number of true positives(TP), false positives(FP), and false negatives(FN).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True Positive(TP): entities that are recognized by NER and match ground truth.&lt;/li&gt;
&lt;li&gt;False Positive(FP): entities that are recognized by NER but do not match ground truth.&lt;/li&gt;
&lt;li&gt;False Negative(FN): entities annotated in the ground truth that are not recognized by NER.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;NER任务需要同时确定&lt;strong&gt;实体边界&lt;/strong&gt;以及**实体类别。**在精确匹配评估中，只有当实体边界以及实体类别同时被精确标出时，实体识别任务才能被认定为成功。基于数据的 true positives（TP），false positives（FP），以及false negatives（FN），可以计算NER任务的精确率，召回率以及 F-score 用于评估任务优劣。对NER中的 true positives（TP），false positives（FP）与false negatives（FN）有如下解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;true positives（TP）：NER能正确识别实体&lt;/li&gt;
&lt;li&gt;false positives（FP）：NER能识别出实体但类别或边界判定出现错误&lt;/li&gt;
&lt;li&gt;false negatives（FN）：应该但没有被NER所识别的实体&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Precision measures the ability of a NER system to present only correct entities, and Recall measures the ability of a NER system to recognize all entities in a corpus.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$Precision=\frac{TP}{TP+FP}$&lt;/p&gt;
&lt;p&gt;$Recall=\frac{TP}{TP+FN}$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;F-score is the harmonic mean of precision and recall, and the balanced F-score is most commonly used:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$Fscore=2\times\frac{Precision\times{Recall}}{Precision+Recall}$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As most of NER systems involve multiple entity types, it is often required to assess the performance across all entity classes. Two measures are commonly used for this purpose:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Macro-averaged F-score&lt;/p&gt;
&lt;p&gt;Computes the F-score independently for each entity type, then takes the average(hence treating all entity types equally).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Micro-averaged F-score&lt;/p&gt;
&lt;p&gt;Aggregates the contributions of entities from all classes to compute the average(treating all entities equally).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;绝大多数的NER任务需要识别多种实体类别，需要对所有的实体类别评估NER的效果。基于这个思路，有两类评估指标：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;宏平均 F-score（macro-averaged F-score）：分别对每种实体类别分别计算对应类别的 F-score，再求整体的平均值（将所有的实体类别都视为平等的）&lt;/li&gt;
&lt;li&gt;微平均 F-score（micro-averaged F-score）：对整体数据求 F-score（将每个实体个体视为平等的）&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;relaxed-match-evaluation&#34;&gt;Relaxed-match Evaluation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;MUC-6 defines a relaxed-match evaluation: a correct type is credited if an entity is assigned its correct type regardless its boundaries as long as there is an overlap with ground truth boundaries; a correct boundary is credited regardless an entity’s type assignment. Thus, complex evaluation methods are not widely used in recent NER studies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;
&lt;a href=&#34;http://p0st3r.github.io/2020/http%3A//dl.acm.org/ft_gateway.cfm%3Fid%3D992709%26type%3Dpdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MUC-6&lt;/a&gt; 定义了一种宽松匹配评估标准：只要实体的边界与实体真正所在的位置有重合（overlap）且实体类别识别无误，就可以认定实体类别识别正确；对实体边界的识别也不用考虑实体类别识别的正确与否。与精确匹配评估相比，宽松匹配评估的应用较少。&lt;/p&gt;
&lt;h2 id=&#34;traditional-approaches-to-ner&#34;&gt;Traditional Approaches to NER&lt;/h2&gt;
&lt;h3 id=&#34;rule-based-approaches&#34;&gt;Rule-based Approaches&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Rule-based NER systems rely on hand-crafted rules. Rules can be designed based on domain-specific gaztteers, and syntactic-lexical patterns.&lt;/p&gt;
&lt;p&gt;These systems are mainly based on hand-crafted semantic and syntactic rules to recognize entities. Rule-based systems work very well when lexicon is exhaustive. Due to domain-specific rules and incomplete dictionaries, high precision and low recall are often observed from such systems, and the systems can not be transferred to other domains.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;基于规则的NER系统依赖于人工制定的规则。规则的设计一般基于句法、语法、词汇的模式以及特定领域的知识等。当字典大小有限时，基于规则的NER系统可以达到很好的效果。由于特定领域的规则以及不完全的字典，这种NER系统的特点是高精确率与低召回率，并且类似的系统难以迁移应用到别的领域中去：基于领域的规则往往不通用，对新的领域而言，需要重新制定规则且不同领域字典不同。&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-learning-approaches&#34;&gt;Unsupervised Learning Approaches&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;A typical approach of unsupervised learning is clustering. Clustering-based NER systems extract named entities from the clustered groups based on context similarity. The key idea is that lexical resources, lexical patterns, and statistics computed on a large corpus can be used to infer mentions of named entities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;典型的无监督方法如聚类可以利用语义相似性，从聚集的组中抽取命名实体。其核心思路在于利用基于巨大语料得到的词汇资源、词汇模型、统计数据来推断命名实体的类别。&lt;/p&gt;
&lt;p&gt;##Feature-based Supervised Learning Approaches&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Applying supervised learning, NER is cast to a multi-class classification or sequence labeling task. Given annotated data samples, features are carefully designed to represent each training example. Machine learning algorithms are then utilized to learn a model to recognize similar patterns from unseen data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;利用监督学习，NER任务可以被转化为多分类任务或者序列标注任务。根据标注好的数据，研究者应用领域知识与工程技巧设计复杂的特征来表征每个训练样本，然后应用机器学习算法，训练模型使其对数据的模式进行学习。&lt;/p&gt;
&lt;h2 id=&#34;deep-learning-techniques-for-ner&#34;&gt;Deep learning Techniques for NER&lt;/h2&gt;
&lt;h3 id=&#34;why-deep-learning-for-ner&#34;&gt;Why Deep Learning for NER?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;There are three core strengths of applying deep learning techniques to NER:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NER benefits from the non-linear transformation, which generates non-linear mappings from input to output. Compared with linear models(e.g., loglinear HMM and linear chain CRF), deep-learning models are able to learn complex and intricate features from data via non-linear activation functions.&lt;/li&gt;
&lt;li&gt;Deep learning saves significant effort on designing NER features. The traditional feature-based approaches require considerable amount of engineering skill and domain expertise. Deep learning models are effective in automatically learning useful representations and underlying factors from raw data.&lt;/li&gt;
&lt;li&gt;Deep neural NER models can be trained in an end-to-end paradigm, by gradient descent. This property enables us to design possibly complex NER systems&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;三个主要的优势：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;NER可以利用深度学习非线性的特点，从输入到输出建立非线性的映射。相比于线性模型（如线性链式CRF、log-linear隐马尔可夫模型），深度学习模型可以利用巨量数据通过非线性激活函数学习得到更加复杂精致的特征。&lt;/li&gt;
&lt;li&gt;深度学习不需要过于复杂的特征工程。传统的基于特征的方法需要大量的工程技巧与领域知识；而深度学习方法可以从输入中自动发掘信息以及学习信息的表示，而且通常这种自动学习并不意味着更差的结果。&lt;/li&gt;
&lt;li&gt;深度NER模型是端到端的；端到端模型的一个好处在于可以避免流水线（pipeline）类模型中模块之间的误差传播；另一点是端到端的模型可以承载更加复杂的内部设计，最终产出更好的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;taxonomy&#34;&gt;Taxonomy&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;The taxonomy of DL-based NER. FRO input sequence to predicted tags, a DL-based NER model consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distributed representations for input&lt;/li&gt;
&lt;li&gt;Context encoder&lt;/li&gt;
&lt;li&gt;Tag decoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;文章针对现有的深度NER模型提出了一种新的归纳方法。这种归纳法将深度NER系统概括性的分为了三个阶段：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输入的分布式表示（distributed representation）&lt;/li&gt;
&lt;li&gt;语境语义编码（context encoder）&lt;/li&gt;
&lt;li&gt;标签解码（tag decoder）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个深度NER系统的结构示例如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;assets/image-20200418111322568.png&#34; alt=&#34;image-20200418111322568&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;distributed-representations-for-input&#34;&gt;Distributed Representations for Input&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;A straight forward option of representing a word is &lt;em&gt;one-hot&lt;/em&gt; vector representation. In one-hot vector space, two distinct words have completely different representations and are orthogonal. &lt;em&gt;Distributed representation&lt;/em&gt; represents words in low dimensional real-valued dense vectors where each dimension represents a latent feature. Automatically learned from text, distributed representation captures semantic and syntactic properties of word, which do not explicitly present in the input to NER.&lt;/p&gt;
&lt;p&gt;Three types of distributed representations that have been used in NER models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Word-level Representation
&lt;ul&gt;
&lt;li&gt;CBOW, continuous bag-of-words&lt;/li&gt;
&lt;li&gt;continuous skip-gram model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Character-level Representation&lt;/li&gt;
&lt;li&gt;Hybrid Representation&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;分布式语义表示&lt;/strong&gt;：一个单词的含义是由这个单词常出现的语境（上下文）所决定的&lt;/p&gt;
&lt;p&gt;一种直接粗暴的单词表示方法为 one-hot 向量表示。这种方法通常向量的维度太大，极度稀疏，且任何两个向量都是正交的，无法用于计算单词相似度。&lt;strong&gt;分布式表示&lt;/strong&gt;使用低维度稠密实值向量表示单词，其中每个维度表示一个隐特征（此类特征由模型自动学习得到，而非人为明确指定，研究者往往不知道这些维度到底代表的是什么具体的特征）。这些分布式表示可以自动地从输入文本中学习得到重要的信息。深度NER模型主要用到了三类分布式表示：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;单词级别表示&lt;/li&gt;
&lt;li&gt;字符级别表示&lt;/li&gt;
&lt;li&gt;混合表示&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;word-level-representation&#34;&gt;Word-level Representation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Usually, each word can be represented by a low-dimensional real value vector after training. Using as the input, the pre-trained word embeddings can be either fixed or further fine-tuned during NER model training.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通常经过训练，每个单词可以用一个低维度的实值向量表示。&lt;/p&gt;
&lt;p&gt;作为后续阶段的输入，这些词嵌入向量既可以在预训练之后就固定，也可以根据具体应用场景进行调整。&lt;/p&gt;
&lt;p&gt;Commonly used word embeddings include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://code.google.com/archive/p/word2vec/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://nlp.stanford.edu/projects/glove/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford GloVe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://fasttext.cc/docs/en/english-vectors.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook fastText&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://ronan.collobert.com/senna/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SENNA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://p0st3r.github.io/2020/https%3A//arxiv.org/abs/1706.05075&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme&lt;/a&gt; use Word2Vec for end-to-end joint extraction model learning to obtain word representations as model input&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://p0st3r.github.io/2020/https%3A//arxiv.org/abs/1702.02098&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast and accurate entity recognition with iterated dilated convolutions&lt;/a&gt; The lookup table in their model are initialized by 100-dimensional embeddings trained on SENNA corpus by skip-n-gram.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;character-level-representation&#34;&gt;Character-level Representation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Character-level representation has been found useful for exploiting explicit sub-word-level information such as prefix and suffix. Another advantage of character-level representation is that it naturally handles out-of-vocabulary. Thus character-based model is able to infer representations for unseen words and share information of morpheme-level regularities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;字符级别的表示能更有效地利用&lt;strong&gt;次词级别&lt;/strong&gt;信息如前缀、后缀等。其另一个好处在于它可以很好地处理 out-of-vocabulary 问题。字符级别的表示可以对没有见过的（训练语料中未曾出现的）单词进行合理推断并给出相应的表示，并在语素层面上共享、处理信息（&lt;strong&gt;语素&lt;/strong&gt;：最小的的音义结合体）。主流的抽取字符级别表示的结构分为：&lt;/p&gt;
&lt;p&gt;There are two widely-used architectures for extracting character-level representation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNN-based&lt;/li&gt;
&lt;li&gt;RNN-based&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figures below illustrate the two architectures:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;assets/image-20200418111407867.png&#34; alt=&#34;image-20200418111407867&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;assets/image-20200418111423908.png&#34; alt=&#34;image-20200418111423908&#34;&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNN-based：
&lt;a href=&#34;http://p0st3r.github.io/2020/https%3A//arxiv.org/abs/1802.05365&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep contextualized word representations(ELMo)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RNN-based：
&lt;a href=&#34;http://p0st3r.github.io/2020/http%3A//www.aclweb.org/anthology/C/C16/C16-1087.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CharNER&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;hybrid-representation&#34;&gt;Hybrid Representation&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Some studies also incorporate additional information(e.g., gazetteers and lexical similarity) into the final representations of words, before feeding into context encoding layers. In other words, the DL-based representation is combined with feature-based approach in a hybrid manner. Adding additional information may lead to improvements in NER performance, with the price of hurting generality of these systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;某些单词表示研究还结合了一些其他信息，例如句法信息、词法信息、领域信息等。这些研究将这些附加的信息与单词表示或字符级别的表示相结合作为最终的单词表示，之后再作为输入输入到后续的语义编码结构当中。换而言之，这种方法的本质是将基于深度学习的单词表示与基于特征的方法相结合。这些额外的信息可能可以提升NER系统的性能，但是代价是可能会降低系统的通用性与可迁移性。&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://p0st3r.github.io/2020/http%3A//arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;#0x07 Context Encoder&lt;/p&gt;
&lt;p&gt;The second stage of DL-based NER is to learn context encoder from the input representations.&lt;/p&gt;
&lt;p&gt;The widely-used context encoder architectures are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Convolutional Neural Networks, CNN&lt;/li&gt;
&lt;li&gt;Recurrent Neural Networks, RNN&lt;/li&gt;
&lt;li&gt;Recursive Neural Networks&lt;/li&gt;
&lt;li&gt;Neural Language Models&lt;/li&gt;
&lt;li&gt;Deep Transformer&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;convolutional-neural-networks&#34;&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;p&gt;Collobert et al. proposed a sentence approach network where a word is tagged with the consideration of whole sentence:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;assets/image-20200418111044886.png&#34; alt=&#34;image-20200418111044886&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Each word in the input sequence is embedded to an N-dimensional vector after the stage of input representation. Then a convolutional layer is used to produce local features around each word, and the size of the output of the convolutional layers depends on the number of words in the sentence. The global feature vector is constructed by combining local feature vectors extracted by the convolutional layers. The dimension of the global feature vector is fixed, independent of the sentence length, in order to apply subsequent standard affine layers. Two approaches are widely used to extract global features: a max or an averaging operation over the position(i.e., “time” step) in the sentence. Finally, these fixed-size global features are fed into tag decoder to compute distribution scores for all possible tags for the words in the network input.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;输入表示阶段，输入序列中的每一个词都被嵌入一个 N 维的向量。在这之后，系统利用卷积神经网络来产生词间的局部特征，并且此时卷积神经网络的输出大小还与输入句子的大小有关。随后，通过对该局部特征施加极大池化（max pooling）或者平均池化（average pooling）操作，我们能得到大小固定且与输入相互独立的全局特征向量。这些长度大小固定的全局特征向量之后将会被导入标签解码结构中，分别对所有可能的标签计算相应的置信分数，完成对标签的预测。&lt;/p&gt;
&lt;h3 id=&#34;recurrent-neural-networks&#34;&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Recurrent neural networks have demonstrated remarkable achievements in modeling sequential data, together with its variants such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gated Recurrent Unit, GRU&lt;/li&gt;
&lt;li&gt;Long-short Term Memory, LSTM&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In particular, bidirectional RNNs efficiently make use of past information(via forward states) and future information(via backward states) for a specific time frame. Thus, a token encoded by a bidirectional RNN will contain evidence from the whole input sentence. Bidirectional RNNs therefore become de facto standard for composing deep context-dependent representations of text.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;循环神经网络在处理序列输入时效果优秀，它有两个最常见的变种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GRU（gated recurrent unit）&lt;/li&gt;
&lt;li&gt;LSTM（long-short term memory）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;特别的，双向循环神经网络（bidirectional RNNs）能同时有效地利用过去的信息和未来的信息，即可以有效利用全局信息。因此，双向循环神经网络逐渐成为解决 NER 这类序列标注任务的标准解法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;assets/image-20200418111123972.png&#34; alt=&#34;image-20200418111123972&#34;&gt;&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://p0st3r.github.io/2020/https%3A//arxiv.org/abs/1508.01991&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bidirectional lstm-crf models for sequence tagging&lt;/a&gt; is among the first to utilize a bidirectional LSTM CRF architecture to sequence tagging tasks (POS, chunking and NER).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recursive-neural-networks&#34;&gt;Recursive Neural Networks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Recursive neural networks are non-linear adaptive models that are able to learn deep structured information, by traversing a given structure in topological order. Named entities are highly related to linguistic constituents, e.g., noun phrases. Typical sequential labeling approaches take little into consideration about phrase structures of sentences, however, recursive neural network can effectively use the structural information to obtain better prediction results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;递归神经网络是一种非线性自适应的模型，它可以学习得到输入的深度结构化信息。命名实体与某些语言成分联系十分紧密，如名词词组。传统的序列标注方法几乎忽略了句子的结构信息（成分间的结构），而递归神经网络能有效的利用这样的结构信息，从而得出更好的预测结果。&lt;/p&gt;
&lt;p&gt;Typical application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://aclanthology.info/papers/D17-1281/d17-1281&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Leveraging linguistic structures for named entity recognition with bidirectional recursive neural networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;assets/image-20200418111202373.png&#34; alt=&#34;image-20200418111202373&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;neural-language-model&#34;&gt;Neural Language Model&lt;/h3&gt;
&lt;p&gt;Language model is a family of models describing the generation of sequences. Given a token sequence, (t1,t2,…,tN)(t1,t2,…,tN), a forward language model computes the probability of sequence by modeling the probability of token tktk given its history t1,…,tk−1t1,…,tk−1:
p(t1,t2,…,tN)=N∏k=1p(tk|t1,t2,…,tk−1)p(t1,t2,…,tN)=∏k=1Np(tk|t1,t2,…,tk−1)
A backward language model is similar to a forward language model, except it runs over the sequence in reverse order, predicting the previous token given its future context:
p(t1,t2,…,tN)=N∏k=1p(tk|tk+1,tk+2,…,tN)p(t1,t2,…,tN)=∏k=1Np(tk|tk+1,tk+2,…,tN)
For neural language models, probability of token tktk can be computed by the output of recurrent neural networks. At each position kk, we can obtain two context-dependent representations (forward and backward) and then combine them as the final language model embedding for token tktk. Such language-model-augmented knowledge has been empirically verified to be helpful in numerous sequence labeling tasks.&lt;/p&gt;
&lt;p&gt;Typical application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1705.00108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semi-supervised sequence tagging with bidirectional language models&lt;/a&gt; 文章认为，利用单词级别表示作为输入来产生上下文表示的循环神经网络往往是在相对较小的标注数据集上训练的。而神经语言模型可以在大型的无标注数据集上训练。文中模型同时使用词嵌入模型与神经语言模型对无监督的语料进行训练，得到两种单词表示；之后模型中省去了将输入向量转化为上下文相关向量的操作，直接结合前面得到的两类单词表示并用于有监督的序列标注任务，简化了模型的结构。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-transfomer&#34;&gt;Deep Transfomer&lt;/h3&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://p0st3r.github.io/2020/https%3A//arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention is all you need&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://p0st3r.github.io/2020/https%3A//arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bert: Pretraining of deep bidirectional transformers for language understanding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tag-decoder&#34;&gt;Tag Decoder&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Tag decoder is the final stage in a NER model. It takes context-dependent representations as input and produce a sequence of tags corresponding to input sequence.&lt;/p&gt;
&lt;p&gt;The paper summarizes four architectures of tag decoder:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;标签解码是NER模型中的最后一个阶段。在得到了单词的向量表示并将它们转化为上下文相关的表示之后，标签解码模块以它们作为输入并对整个模型的输入预测相应的标签序列。主流的标签解码结构分为四类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-Layer Perceptron(MLP) + Softmax&lt;/li&gt;
&lt;li&gt;Conditional Random Fields, CRFs&lt;/li&gt;
&lt;li&gt;Recurrent Neural Networks&lt;/li&gt;
&lt;li&gt;Pointer Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-layer-perceptron--softmax&#34;&gt;Multi-Layer Perceptron + Softmax&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;With a multi-layer Perceptron + Softmax layer as the tag decoder layer, the sequence labeling task is cast as a multi-class classification problem. Tag for each word is independently predicted based on the context-dependent representations without taking into account its neighbors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;利用这个结构可以将NER这类序列标注模型视为多类型分类问题。基于该阶段输入的上下文语义表示，每个单词的标签被独立地预测，与其邻居无关。&lt;/p&gt;
&lt;p&gt;Typical application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1702.02098&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast and accurate entity recognition with iterated dilated convolutions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Leveraging linguistic structures for named entity recognition with bidirectional recursive neural networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conditional-random-fields&#34;&gt;Conditional Random Fields&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;A conditional random field(CRF) is a random field globally conditioned on the observation sequence. CRFs have been widely used in feature-based supervised learning approaches. Many deep learning based NER models use a CRF layer as the tag decoder.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;条件随机场（conditional random fields）是一类概率图模型，在基于特征的有监督方法中应用广泛，近来的许多深度学习方法也使用条件随机场作为最后的标签标注结构。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CRFs, however, cannot make full use of segment-level information because the inner properties of segments cannot be fully encoded with word-level representations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然而，CRFs不能充分利用段级信息，因为段的内部属性不能完全用字级表示进行编码。&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1508.01991&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bidirectional lstm-crf models for sequence tagging&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recurrent-neural-networks-1&#34;&gt;Recurrent Neural Networks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;A few studies have explored RNN to decode tags.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.05928&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep active learning for named entity recognition&lt;/a&gt; reported that RNN tag decoders outperform CRF and are faster to train when the number of entity types is large&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一些研究使用 RNN 来预测标签。&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.05928&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep active learning for named entity recognition&lt;/a&gt;这篇文章中提到，RNN 模型作为预测标签的解码器性能优于 CRF，并且当实体类型很多的时候训练速度更快&lt;/p&gt;
&lt;h3 id=&#34;point-networks&#34;&gt;Point Networks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Pointer networks apply RNNs to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to the positions in an input sequence. It represents variable length dictionaries by using a softmax probability distribution as a “pointer”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/pdf/1506.03134v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pointer Networks&lt;/a&gt; first applied pointer networks to produce sequence tag&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/pdf/1701.04027&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Models for Sequence Chunking&lt;/a&gt; first identify a chunk (or a segment), and then label it. This operation is repeated until all the words in input sequence are processed&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;指针网络应用RNNs来学习输出序列的条件概率，其中输出序列中的元素是与输入序列中的位置相对应的离散标记。它使用softmax概率分布作为一个“指针”来表示可变长度字典&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/pdf/1506.03134v1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pointer Networks&lt;/a&gt; 首次提出此种结构&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/pdf/1701.04027&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Models for Sequence Chunking&lt;/a&gt; 第一篇将pointer networks结构应用到生成序列标签任务中的文章&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
